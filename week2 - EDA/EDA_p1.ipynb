{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mud card\n",
    "- **When we want to find hyper parameters, we use embedded loops to go through it, but the problem I have is that how that related to cross-validation methods, since right now, we have a concrete training set and validation sets in order to get a good hyper parameters. From my personal knowledge, cross-validation method would change training set a few times, so I confused about what is the connections between them**\n",
    "    - what you refer to is called k-fold cross validation\n",
    "    - that's just one way to do CV\n",
    "    - we will cover a handful of techniques to do CV/tune hyperparameters\n",
    "- **In quiz 3, the option \"Underfitting means that the model performs similarly on the training and validation sets\", why is it incorrect? In the example you have shown in class, underfitting means that models perform poorly on both the training and validation sets.**\n",
    "- **for the quiz 3, underfit means they both perform poorly, why they can't be said be similar?**\n",
    "    -  the best model can perform similarly on the training and validation sets, and that model performance is not poor, it's optimal\n",
    "    - you need to see the whole training and validation curves as a function of the hyperparameter to decide when the model performance is poor\n",
    "- **For a certain model, such as gradient descent model, I usually change the step size manually, how can I choose the step size in a \"smarter\" way?**\n",
    "    - gradient descent is a numerical algorithm used to find a local or global minimum of a function\n",
    "    - it is used in ML algorithms to find optimal model parameters\n",
    "    - but it is not a ML model\n",
    "- **Are there any resources we can use to practice the things we're learning in class that aren't graded? So like some practice exercises?**\n",
    "    - kaggle.com\n",
    "    - check out and participate in ML comeptitions\n",
    "- **Just the last contour plot - I am interested in knowing more about how the background was colored**\n",
    "- **I'm not familiar with some package/function we use in python**\n",
    "    - I unfortunately don't have time to go through the code line by line during class so I highly recommend that you study the code outside of class\n",
    "    - print out the variables\n",
    "    - read the manuals of the functions\n",
    "    - change things in the code\n",
    "- **Where and how \"Cs = np.logspace(-1,3,13)\" is developed is the muddiest for me.**\n",
    "    - check out help(np.logspace)\n",
    "    - it's a numpy function that generates uniformly spaced numbers in log space\n",
    "    - I'll show you the pythonian tricks I came across and found useful\n",
    "    - you'll find your own tricks and favorite functions\n",
    "- **Going forward, will the mathematical definitions for some of these ideas be provided? I'm probably in the minority for preferrring this, but I think that it helps to see those definitions, even if they aren't talked about. For example \"A dataset is structured if all elements can be minimally embedded in R^d for the same d\" or \"A dataset is unstructured if the minimal embedding for elements vary\"**\n",
    "    - not so much in this class\n",
    "    - we will focus on practical issues rather than rigorous mathematical formulation\n",
    "    - Sam is the mathematician :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Exploratory data analysis in python, part 1 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The steps\n",
    "\n",
    "**1. Exploratory Data Analysis (EDA)**: you need to understand your data and verify that it doesn't contain errors\n",
    "   - do as much EDA as you can!\n",
    "    \n",
    "**2. Split the data into different sets**: most often the sets are train, validation, and test (or holdout)\n",
    "   - practitioners often make errors in this step!\n",
    "   - you can split the data randomly, based on groups, based on time, or any other non-standard way if necessary to answer your ML question\n",
    "\n",
    "**3. Preprocess the data**: ML models only work if X and Y are numbers! Some ML models additionally require each feature to have 0 mean and 1 standard deviation (standardized features)\n",
    "   - often the original features you get contain strings (for example a gender feature would contain 'male', 'female', 'non-binary', 'unknown') which needs to transformed into numbers\n",
    "   - often the features are not standardized (e.g., age is between 0 and 100) but it needs to be standardized\n",
    "    \n",
    "**4. Choose an evaluation metric**: depends on the priorities of the stakeholders\n",
    "   - often requires quite a bit of thinking and ethical considerations\n",
    "     \n",
    "**5. Choose one or more ML techniques**: it is highly recommended that you try multiple models\n",
    "   - start with simple models like linear or logistic regression\n",
    "   - try also more complex models like nearest neighbors, support vector machines, random forest, etc.\n",
    "    \n",
    "**6. Tune the hyperparameters of your ML models (aka cross-validation)**\n",
    "   - ML techniques have hyperparameters that you need to optimize to achieve best performance\n",
    "   - for each ML model, decide which parameters to tune and what values to try\n",
    "   - loop through each parameter combination\n",
    "       - train one model for each parameter combination\n",
    "       - evaluate how well the model performs on the validation set\n",
    "   - take the parameter combo that gives the best validation score\n",
    "   - evaluate that model on the test set to report how well the model is expected to perform on previously unseen data\n",
    "    \n",
    "**7. Interpret your model**: black boxes are often not useful\n",
    "   - check if your model uses features that make sense (excellent tool for debugging)\n",
    "   - often model predictions are not enough, you need to be able to explain how the model arrived to a particular prediction (e.g., in health care)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Pandas </center>\n",
    "\n",
    "- data are often distributed over multiple files/databases (e.g., csv and excel files, sql databases)\n",
    "- each file/database is read into a pandas dataframe\n",
    "- you often need to filter dataframes (select specific rows/columns based on index or condition)\n",
    "- pandas dataframes can be merged and appended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some notes and advice\n",
    "\n",
    "- **ALWAYS READ THE HELP OF THE METHODS/FUNCTIONS YOU USE!**\n",
    "\n",
    "- stackoverflow is your friend, use it! https://stackoverflow.com/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Data transformations: pandas data frames </center>\n",
    "### By the end of this lecture, you will be able to\n",
    "   - read in csv, excel, and sql data into a pandas data frame\n",
    "   - filter rows in various ways\n",
    "   - select columns\n",
    "   - merge and append data frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <font color='LIGHTGRAY'><center> Data transformations: pandas data frames </center></font>\n",
    "###  <font color='LIGHTGRAY'>By the end of this lecture, you will be able to</font>\n",
    "   - **read in csv, excel, and sql data into a pandas data frame**\n",
    "   -  <font color='LIGHTGRAY'>filter rows in various ways</font>\n",
    "   -  <font color='LIGHTGRAY'>select columns</font>\n",
    "   -  <font color='LIGHTGRAY'>merge and append data frames</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age          workclass  fnlwgt   education  education-num  \\\n",
      "0   39          State-gov   77516   Bachelors             13   \n",
      "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
      "2   38            Private  215646     HS-grad              9   \n",
      "3   53            Private  234721        11th              7   \n",
      "4   28            Private  338409   Bachelors             13   \n",
      "\n",
      "        marital-status          occupation    relationship    race      sex  \\\n",
      "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
      "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
      "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
      "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
      "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
      "\n",
      "   capital-gain  capital-loss  hours-per-week  native-country gross-income  \n",
      "0          2174             0              40   United-States        <=50K  \n",
      "1             0             0              13   United-States        <=50K  \n",
      "2             0             0              40   United-States        <=50K  \n",
      "3             0             0              40   United-States        <=50K  \n",
      "4             0             0              40            Cuba        <=50K  \n"
     ]
    }
   ],
   "source": [
    "# how to read in a database into a dataframe and basic dataframe structure\n",
    "import pandas as pd\n",
    "\n",
    "# load data from a csv file\n",
    "df = pd.read_csv('data/adult_data.csv') # there are also pd.read_excel(), and pd.read_sql()\n",
    "\n",
    "#print(df)\n",
    "print(df.head()) # by default, shows the first five rows but check help(df.head) to specify the number of rows to show\n",
    "#print(df.shape) # the shape of your dataframe (number of rows, number of columns)\n",
    "#print(df.shape[0]) # number of rows\n",
    "#print(df.shape[1]) # number of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Packages\n",
    "\n",
    "A package is a collection of classes and functions.\n",
    "- a dataframe (pd.DataFrame()) is a pandas class\n",
    "    - a class is the blueprint of how the data should be organized \n",
    "    - classes have methods which can perform operations on the data (e.g., .head(), .shape)\n",
    "- df is an object, an instance of the class.\n",
    "    - we put data into the class \n",
    "    - methods are attached to objects \n",
    "       - you cannot call pd.head(), you can only call df.head()\n",
    "- read_csv is a function\n",
    "    - functions are called from the package\n",
    "    - you cannot call df.read_csv, you can only call pd.read_csv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### DataFrame structure: both rows and columns are indexed!\n",
    "- index column, no name\n",
    "    - contains the row names\n",
    "    - by default, index is a range object from 0 to number of rows - 1 \n",
    "    - any column can be turned into an index, so indices can be non-number, and also non-unique. more on this later.\n",
    "- columns with column names on top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Always print your dataframe to check if it looks ok!\n",
    "\n",
    "### Most common reasons it might not look ok:\n",
    "\n",
    "   - the first row is not the column name\n",
    "        - there are rows above the column names that need to be skipped\n",
    "        - there is no column name but by default, pandas assumes the first row is the column name. as a result, \n",
    "          the values of the first row end up as column names.\n",
    "   - character encoding is off\n",
    "   - separator is not comma but some other charachter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function read_csv in module pandas.io.parsers.readers:\n",
      "\n",
      "read_csv(filepath_or_buffer: 'FilePathOrBuffer', sep=<no_default>, delimiter=None, header='infer', names=<no_default>, index_col=None, usecols=None, squeeze=False, prefix=<no_default>, mangle_dupe_cols=True, dtype: 'DtypeArg | None' = None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, skipfooter=0, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=False, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False, cache_dates=True, iterator=False, chunksize=None, compression='infer', thousands=None, decimal: 'str' = '.', lineterminator=None, quotechar='\"', quoting=0, doublequote=True, escapechar=None, comment=None, encoding=None, encoding_errors: 'str | None' = 'strict', dialect=None, error_bad_lines=None, warn_bad_lines=None, on_bad_lines=None, delim_whitespace=False, low_memory=True, memory_map=False, float_precision=None, storage_options: 'StorageOptions' = None)\n",
      "    Read a comma-separated values (csv) file into DataFrame.\n",
      "    \n",
      "    Also supports optionally iterating or breaking of the file\n",
      "    into chunks.\n",
      "    \n",
      "    Additional help can be found in the online docs for\n",
      "    `IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    filepath_or_buffer : str, path object or file-like object\n",
      "        Any valid string path is acceptable. The string could be a URL. Valid\n",
      "        URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n",
      "        expected. A local file could be: file://localhost/path/to/table.csv.\n",
      "    \n",
      "        If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n",
      "    \n",
      "        By file-like object, we refer to objects with a ``read()`` method, such as\n",
      "        a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\n",
      "    sep : str, default ','\n",
      "        Delimiter to use. If sep is None, the C engine cannot automatically detect\n",
      "        the separator, but the Python parsing engine can, meaning the latter will\n",
      "        be used and automatically detect the separator by Python's builtin sniffer\n",
      "        tool, ``csv.Sniffer``. In addition, separators longer than 1 character and\n",
      "        different from ``'\\s+'`` will be interpreted as regular expressions and\n",
      "        will also force the use of the Python parsing engine. Note that regex\n",
      "        delimiters are prone to ignoring quoted data. Regex example: ``'\\r\\t'``.\n",
      "    delimiter : str, default ``None``\n",
      "        Alias for sep.\n",
      "    header : int, list of int, default 'infer'\n",
      "        Row number(s) to use as the column names, and the start of the\n",
      "        data.  Default behavior is to infer the column names: if no names\n",
      "        are passed the behavior is identical to ``header=0`` and column\n",
      "        names are inferred from the first line of the file, if column\n",
      "        names are passed explicitly then the behavior is identical to\n",
      "        ``header=None``. Explicitly pass ``header=0`` to be able to\n",
      "        replace existing names. The header can be a list of integers that\n",
      "        specify row locations for a multi-index on the columns\n",
      "        e.g. [0,1,3]. Intervening rows that are not specified will be\n",
      "        skipped (e.g. 2 in this example is skipped). Note that this\n",
      "        parameter ignores commented lines and empty lines if\n",
      "        ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n",
      "        data rather than the first line of the file.\n",
      "    names : array-like, optional\n",
      "        List of column names to use. If the file contains a header row,\n",
      "        then you should explicitly pass ``header=0`` to override the column names.\n",
      "        Duplicates in this list are not allowed.\n",
      "    index_col : int, str, sequence of int / str, or False, default ``None``\n",
      "      Column(s) to use as the row labels of the ``DataFrame``, either given as\n",
      "      string name or column index. If a sequence of int / str is given, a\n",
      "      MultiIndex is used.\n",
      "    \n",
      "      Note: ``index_col=False`` can be used to force pandas to *not* use the first\n",
      "      column as the index, e.g. when you have a malformed file with delimiters at\n",
      "      the end of each line.\n",
      "    usecols : list-like or callable, optional\n",
      "        Return a subset of the columns. If list-like, all elements must either\n",
      "        be positional (i.e. integer indices into the document columns) or strings\n",
      "        that correspond to column names provided either by the user in `names` or\n",
      "        inferred from the document header row(s). For example, a valid list-like\n",
      "        `usecols` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n",
      "        Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n",
      "        To instantiate a DataFrame from ``data`` with element order preserved use\n",
      "        ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]`` for columns\n",
      "        in ``['foo', 'bar']`` order or\n",
      "        ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n",
      "        for ``['bar', 'foo']`` order.\n",
      "    \n",
      "        If callable, the callable function will be evaluated against the column\n",
      "        names, returning names where the callable function evaluates to True. An\n",
      "        example of a valid callable argument would be ``lambda x: x.upper() in\n",
      "        ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n",
      "        parsing time and lower memory usage.\n",
      "    squeeze : bool, default False\n",
      "        If the parsed data only contains one column then return a Series.\n",
      "    prefix : str, optional\n",
      "        Prefix to add to column numbers when no header, e.g. 'X' for X0, X1, ...\n",
      "    mangle_dupe_cols : bool, default True\n",
      "        Duplicate columns will be specified as 'X', 'X.1', ...'X.N', rather than\n",
      "        'X'...'X'. Passing in False will cause data to be overwritten if there\n",
      "        are duplicate names in the columns.\n",
      "    dtype : Type name or dict of column -> type, optional\n",
      "        Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32,\n",
      "        'c': 'Int64'}\n",
      "        Use `str` or `object` together with suitable `na_values` settings\n",
      "        to preserve and not interpret dtype.\n",
      "        If converters are specified, they will be applied INSTEAD\n",
      "        of dtype conversion.\n",
      "    engine : {'c', 'python'}, optional\n",
      "        Parser engine to use. The C engine is faster while the python engine is\n",
      "        currently more feature-complete.\n",
      "    converters : dict, optional\n",
      "        Dict of functions for converting values in certain columns. Keys can either\n",
      "        be integers or column labels.\n",
      "    true_values : list, optional\n",
      "        Values to consider as True.\n",
      "    false_values : list, optional\n",
      "        Values to consider as False.\n",
      "    skipinitialspace : bool, default False\n",
      "        Skip spaces after delimiter.\n",
      "    skiprows : list-like, int or callable, optional\n",
      "        Line numbers to skip (0-indexed) or number of lines to skip (int)\n",
      "        at the start of the file.\n",
      "    \n",
      "        If callable, the callable function will be evaluated against the row\n",
      "        indices, returning True if the row should be skipped and False otherwise.\n",
      "        An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\n",
      "    skipfooter : int, default 0\n",
      "        Number of lines at bottom of file to skip (Unsupported with engine='c').\n",
      "    nrows : int, optional\n",
      "        Number of rows of file to read. Useful for reading pieces of large files.\n",
      "    na_values : scalar, str, list-like, or dict, optional\n",
      "        Additional strings to recognize as NA/NaN. If dict passed, specific\n",
      "        per-column NA values.  By default the following values are interpreted as\n",
      "        NaN: '', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan',\n",
      "        '1.#IND', '1.#QNAN', '<NA>', 'N/A', 'NA', 'NULL', 'NaN', 'n/a',\n",
      "        'nan', 'null'.\n",
      "    keep_default_na : bool, default True\n",
      "        Whether or not to include the default NaN values when parsing the data.\n",
      "        Depending on whether `na_values` is passed in, the behavior is as follows:\n",
      "    \n",
      "        * If `keep_default_na` is True, and `na_values` are specified, `na_values`\n",
      "          is appended to the default NaN values used for parsing.\n",
      "        * If `keep_default_na` is True, and `na_values` are not specified, only\n",
      "          the default NaN values are used for parsing.\n",
      "        * If `keep_default_na` is False, and `na_values` are specified, only\n",
      "          the NaN values specified `na_values` are used for parsing.\n",
      "        * If `keep_default_na` is False, and `na_values` are not specified, no\n",
      "          strings will be parsed as NaN.\n",
      "    \n",
      "        Note that if `na_filter` is passed in as False, the `keep_default_na` and\n",
      "        `na_values` parameters will be ignored.\n",
      "    na_filter : bool, default True\n",
      "        Detect missing value markers (empty strings and the value of na_values). In\n",
      "        data without any NAs, passing na_filter=False can improve the performance\n",
      "        of reading a large file.\n",
      "    verbose : bool, default False\n",
      "        Indicate number of NA values placed in non-numeric columns.\n",
      "    skip_blank_lines : bool, default True\n",
      "        If True, skip over blank lines rather than interpreting as NaN values.\n",
      "    parse_dates : bool or list of int or names or list of lists or dict, default False\n",
      "        The behavior is as follows:\n",
      "    \n",
      "        * boolean. If True -> try parsing the index.\n",
      "        * list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\n",
      "          each as a separate date column.\n",
      "        * list of lists. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as\n",
      "          a single date column.\n",
      "        * dict, e.g. {'foo' : [1, 3]} -> parse columns 1, 3 as date and call\n",
      "          result 'foo'\n",
      "    \n",
      "        If a column or index cannot be represented as an array of datetimes,\n",
      "        say because of an unparsable value or a mixture of timezones, the column\n",
      "        or index will be returned unaltered as an object data type. For\n",
      "        non-standard datetime parsing, use ``pd.to_datetime`` after\n",
      "        ``pd.read_csv``. To parse an index or column with a mixture of timezones,\n",
      "        specify ``date_parser`` to be a partially-applied\n",
      "        :func:`pandas.to_datetime` with ``utc=True``. See\n",
      "        :ref:`io.csv.mixed_timezones` for more.\n",
      "    \n",
      "        Note: A fast-path exists for iso8601-formatted dates.\n",
      "    infer_datetime_format : bool, default False\n",
      "        If True and `parse_dates` is enabled, pandas will attempt to infer the\n",
      "        format of the datetime strings in the columns, and if it can be inferred,\n",
      "        switch to a faster method of parsing them. In some cases this can increase\n",
      "        the parsing speed by 5-10x.\n",
      "    keep_date_col : bool, default False\n",
      "        If True and `parse_dates` specifies combining multiple columns then\n",
      "        keep the original columns.\n",
      "    date_parser : function, optional\n",
      "        Function to use for converting a sequence of string columns to an array of\n",
      "        datetime instances. The default uses ``dateutil.parser.parser`` to do the\n",
      "        conversion. Pandas will try to call `date_parser` in three different ways,\n",
      "        advancing to the next if an exception occurs: 1) Pass one or more arrays\n",
      "        (as defined by `parse_dates`) as arguments; 2) concatenate (row-wise) the\n",
      "        string values from the columns defined by `parse_dates` into a single array\n",
      "        and pass that; and 3) call `date_parser` once for each row using one or\n",
      "        more strings (corresponding to the columns defined by `parse_dates`) as\n",
      "        arguments.\n",
      "    dayfirst : bool, default False\n",
      "        DD/MM format dates, international and European format.\n",
      "    cache_dates : bool, default True\n",
      "        If True, use a cache of unique, converted dates to apply the datetime\n",
      "        conversion. May produce significant speed-up when parsing duplicate\n",
      "        date strings, especially ones with timezone offsets.\n",
      "    \n",
      "        .. versionadded:: 0.25.0\n",
      "    iterator : bool, default False\n",
      "        Return TextFileReader object for iteration or getting chunks with\n",
      "        ``get_chunk()``.\n",
      "    \n",
      "        .. versionchanged:: 1.2\n",
      "    \n",
      "           ``TextFileReader`` is a context manager.\n",
      "    chunksize : int, optional\n",
      "        Return TextFileReader object for iteration.\n",
      "        See the `IO Tools docs\n",
      "        <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n",
      "        for more information on ``iterator`` and ``chunksize``.\n",
      "    \n",
      "        .. versionchanged:: 1.2\n",
      "    \n",
      "           ``TextFileReader`` is a context manager.\n",
      "    compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n",
      "        For on-the-fly decompression of on-disk data. If 'infer' and\n",
      "        `filepath_or_buffer` is path-like, then detect compression from the\n",
      "        following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no\n",
      "        decompression). If using 'zip', the ZIP file must contain only one data\n",
      "        file to be read in. Set to None for no decompression.\n",
      "    thousands : str, optional\n",
      "        Thousands separator.\n",
      "    decimal : str, default '.'\n",
      "        Character to recognize as decimal point (e.g. use ',' for European data).\n",
      "    lineterminator : str (length 1), optional\n",
      "        Character to break file into lines. Only valid with C parser.\n",
      "    quotechar : str (length 1), optional\n",
      "        The character used to denote the start and end of a quoted item. Quoted\n",
      "        items can include the delimiter and it will be ignored.\n",
      "    quoting : int or csv.QUOTE_* instance, default 0\n",
      "        Control field quoting behavior per ``csv.QUOTE_*`` constants. Use one of\n",
      "        QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).\n",
      "    doublequote : bool, default ``True``\n",
      "       When quotechar is specified and quoting is not ``QUOTE_NONE``, indicate\n",
      "       whether or not to interpret two consecutive quotechar elements INSIDE a\n",
      "       field as a single ``quotechar`` element.\n",
      "    escapechar : str (length 1), optional\n",
      "        One-character string used to escape other characters.\n",
      "    comment : str, optional\n",
      "        Indicates remainder of line should not be parsed. If found at the beginning\n",
      "        of a line, the line will be ignored altogether. This parameter must be a\n",
      "        single character. Like empty lines (as long as ``skip_blank_lines=True``),\n",
      "        fully commented lines are ignored by the parameter `header` but not by\n",
      "        `skiprows`. For example, if ``comment='#'``, parsing\n",
      "        ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in 'a,b,c' being\n",
      "        treated as the header.\n",
      "    encoding : str, optional\n",
      "        Encoding to use for UTF when reading/writing (ex. 'utf-8'). `List of Python\n",
      "        standard encodings\n",
      "        <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n",
      "    \n",
      "        .. versionchanged:: 1.2\n",
      "    \n",
      "           When ``encoding`` is ``None``, ``errors=\"replace\"`` is passed to\n",
      "           ``open()``. Otherwise, ``errors=\"strict\"`` is passed to ``open()``.\n",
      "           This behavior was previously only the case for ``engine=\"python\"``.\n",
      "    \n",
      "        .. versionchanged:: 1.3.0\n",
      "    \n",
      "           ``encoding_errors`` is a new argument. ``encoding`` has no longer an\n",
      "           influence on how encoding errors are handled.\n",
      "    \n",
      "    encoding_errors : str, optional, default \"strict\"\n",
      "        How encoding errors are treated. `List of possible values\n",
      "        <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n",
      "    \n",
      "        .. versionadded:: 1.3.0\n",
      "    \n",
      "    dialect : str or csv.Dialect, optional\n",
      "        If provided, this parameter will override values (default or not) for the\n",
      "        following parameters: `delimiter`, `doublequote`, `escapechar`,\n",
      "        `skipinitialspace`, `quotechar`, and `quoting`. If it is necessary to\n",
      "        override values, a ParserWarning will be issued. See csv.Dialect\n",
      "        documentation for more details.\n",
      "    error_bad_lines : bool, default ``None``\n",
      "        Lines with too many fields (e.g. a csv line with too many commas) will by\n",
      "        default cause an exception to be raised, and no DataFrame will be returned.\n",
      "        If False, then these \"bad lines\" will be dropped from the DataFrame that is\n",
      "        returned.\n",
      "    \n",
      "        .. deprecated:: 1.3.0\n",
      "           The ``on_bad_lines`` parameter should be used instead to specify behavior upon\n",
      "           encountering a bad line instead.\n",
      "    warn_bad_lines : bool, default ``None``\n",
      "        If error_bad_lines is False, and warn_bad_lines is True, a warning for each\n",
      "        \"bad line\" will be output.\n",
      "    \n",
      "        .. deprecated:: 1.3.0\n",
      "           The ``on_bad_lines`` parameter should be used instead to specify behavior upon\n",
      "           encountering a bad line instead.\n",
      "    on_bad_lines : {'error', 'warn', 'skip'}, default 'error'\n",
      "        Specifies what to do upon encountering a bad line (a line with too many fields).\n",
      "        Allowed values are :\n",
      "    \n",
      "            - 'error', raise an Exception when a bad line is encountered.\n",
      "            - 'warn', raise a warning when a bad line is encountered and skip that line.\n",
      "            - 'skip', skip bad lines without raising or warning when they are encountered.\n",
      "    \n",
      "        .. versionadded:: 1.3.0\n",
      "    \n",
      "    delim_whitespace : bool, default False\n",
      "        Specifies whether or not whitespace (e.g. ``' '`` or ``'    '``) will be\n",
      "        used as the sep. Equivalent to setting ``sep='\\s+'``. If this option\n",
      "        is set to True, nothing should be passed in for the ``delimiter``\n",
      "        parameter.\n",
      "    low_memory : bool, default True\n",
      "        Internally process the file in chunks, resulting in lower memory use\n",
      "        while parsing, but possibly mixed type inference.  To ensure no mixed\n",
      "        types either set False, or specify the type with the `dtype` parameter.\n",
      "        Note that the entire file is read into a single DataFrame regardless,\n",
      "        use the `chunksize` or `iterator` parameter to return the data in chunks.\n",
      "        (Only valid with C parser).\n",
      "    memory_map : bool, default False\n",
      "        If a filepath is provided for `filepath_or_buffer`, map the file object\n",
      "        directly onto memory and access the data directly from there. Using this\n",
      "        option can improve performance because there is no longer any I/O overhead.\n",
      "    float_precision : str, optional\n",
      "        Specifies which converter the C engine should use for floating-point\n",
      "        values. The options are ``None`` or 'high' for the ordinary converter,\n",
      "        'legacy' for the original lower precision pandas converter, and\n",
      "        'round_trip' for the round-trip converter.\n",
      "    \n",
      "        .. versionchanged:: 1.2\n",
      "    \n",
      "    storage_options : dict, optional\n",
      "        Extra options that make sense for a particular storage connection, e.g.\n",
      "        host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
      "        are forwarded to ``urllib`` as header options. For other URLs (e.g.\n",
      "        starting with \"s3://\", and \"gcs://\") the key-value pairs are forwarded to\n",
      "        ``fsspec``. Please see ``fsspec`` and ``urllib`` for more details.\n",
      "    \n",
      "        .. versionadded:: 1.2\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    DataFrame or TextParser\n",
      "        A comma-separated values (csv) file is returned as two-dimensional\n",
      "        data structure with labeled axes.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    DataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\n",
      "    read_csv : Read a comma-separated values (csv) file into DataFrame.\n",
      "    read_fwf : Read a table of fixed-width formatted lines into DataFrame.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> pd.read_csv('data.csv')  # doctest: +SKIP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the help to find the solution\n",
    "help(pd.read_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 1\n",
    "\n",
    "How should we read in adult_test.csv properly? Identify and fix the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('data/adult_test.csv')\n",
    "# print(df.head()) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <font color='LIGHTGRAY'><center> Data transformations: pandas data frames </center></font>\n",
    "###  <font color='LIGHTGRAY'>By the end of this lecture, you will be able to</font>\n",
    "   - <font color='LIGHTGRAY'>read in csv, excel, and sql data into a pandas data frame</font>\n",
    "   -  **filter rows in various ways**\n",
    "   -  <font color='LIGHTGRAY'>select columns</font>\n",
    "   -  <font color='LIGHTGRAY'>merge and append data frames</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How to select rows?\n",
    "\n",
    "##### 1) Integer-based indexing, numpy arrays are indexed the same way.\n",
    "##### 2) Select rows based on the value of the index column\n",
    "##### 3) select rows based on column condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1) Integer-based indexing, numpy arrays are indexed the same way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                                52\n",
      "workclass                Self-emp-inc\n",
      "fnlwgt                         287927\n",
      "education                     HS-grad\n",
      "education-num                       9\n",
      "marital-status     Married-civ-spouse\n",
      "occupation            Exec-managerial\n",
      "relationship                     Wife\n",
      "race                            White\n",
      "sex                            Female\n",
      "capital-gain                    15024\n",
      "capital-loss                        0\n",
      "hours-per-week                     40\n",
      "native-country          United-States\n",
      "gross-income                     >50K\n",
      "Name: 32560, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# df.iloc[] - for more info, see https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#indexing-integer\n",
    "# iloc is how numpy arrays are indexed (non-standard python indexing)\n",
    "\n",
    "# [start:stop:step] -  general indexing format\n",
    "\n",
    "# start stop step are optional\n",
    "#print(df.iloc[:])\n",
    "#print(df.iloc[::])\n",
    "#print(df.iloc[::1])\n",
    "\n",
    "# select one row - 0-based indexing\n",
    "#print(df.iloc[3])\n",
    "\n",
    "# indexing from the end of the data frame\n",
    "print(df.iloc[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age          workclass  fnlwgt   education  education-num  \\\n",
      "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
      "4   28            Private  338409   Bachelors             13   \n",
      "9   42            Private  159449   Bachelors             13   \n",
      "\n",
      "        marital-status        occupation relationship    race      sex  \\\n",
      "1   Married-civ-spouse   Exec-managerial      Husband   White     Male   \n",
      "4   Married-civ-spouse    Prof-specialty         Wife   Black   Female   \n",
      "9   Married-civ-spouse   Exec-managerial      Husband   White     Male   \n",
      "\n",
      "   capital-gain  capital-loss  hours-per-week  native-country gross-income  \n",
      "1             0             0              13   United-States        <=50K  \n",
      "4             0             0              40            Cuba        <=50K  \n",
      "9          5178             0              40   United-States         >50K  \n"
     ]
    }
   ],
   "source": [
    "# select a slice - stop index not included\n",
    "#print(df.iloc[3:7])\n",
    "\n",
    "# select every second element of the slice - stop index not included\n",
    "#print(df.iloc[3:7:2])\n",
    "\n",
    "#print(df.iloc[3:7:-2]) # return empty dataframe\n",
    "#print(df.iloc[7:3:-2])#  return rows with indices 7 and 5. 3 is the stop so it is not included\n",
    "\n",
    "# can be used to reverse rows\n",
    "#print(df.iloc[::-1])\n",
    "\n",
    "# here is where indexing gets non-standard python\n",
    "# select the 2nd, 5th, and 10th rows\n",
    "print(df.iloc[[1,4,9]]) # such indexing doesn't work with lists but it works with numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### 2) Select rows based on the value of the index column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=32561, step=1)\n",
      "Help on method set_index in module pandas.core.frame:\n",
      "\n",
      "set_index(keys, drop: 'bool' = True, append: 'bool' = False, inplace: 'bool' = False, verify_integrity: 'bool' = False) method of pandas.core.frame.DataFrame instance\n",
      "    Set the DataFrame index using existing columns.\n",
      "    \n",
      "    Set the DataFrame index (row labels) using one or more existing\n",
      "    columns or arrays (of the correct length). The index can replace the\n",
      "    existing index or expand on it.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    keys : label or array-like or list of labels/arrays\n",
      "        This parameter can be either a single column key, a single array of\n",
      "        the same length as the calling DataFrame, or a list containing an\n",
      "        arbitrary combination of column keys and arrays. Here, \"array\"\n",
      "        encompasses :class:`Series`, :class:`Index`, ``np.ndarray``, and\n",
      "        instances of :class:`~collections.abc.Iterator`.\n",
      "    drop : bool, default True\n",
      "        Delete columns to be used as the new index.\n",
      "    append : bool, default False\n",
      "        Whether to append columns to existing index.\n",
      "    inplace : bool, default False\n",
      "        If True, modifies the DataFrame in place (do not create a new object).\n",
      "    verify_integrity : bool, default False\n",
      "        Check the new index for duplicates. Otherwise defer the check until\n",
      "        necessary. Setting to False will improve the performance of this\n",
      "        method.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    DataFrame or None\n",
      "        Changed row labels or None if ``inplace=True``.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    DataFrame.reset_index : Opposite of set_index.\n",
      "    DataFrame.reindex : Change to new indices or expand indices.\n",
      "    DataFrame.reindex_like : Change to same indices as other DataFrame.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = pd.DataFrame({'month': [1, 4, 7, 10],\n",
      "    ...                    'year': [2012, 2014, 2013, 2014],\n",
      "    ...                    'sale': [55, 40, 84, 31]})\n",
      "    >>> df\n",
      "       month  year  sale\n",
      "    0      1  2012    55\n",
      "    1      4  2014    40\n",
      "    2      7  2013    84\n",
      "    3     10  2014    31\n",
      "    \n",
      "    Set the index to become the 'month' column:\n",
      "    \n",
      "    >>> df.set_index('month')\n",
      "           year  sale\n",
      "    month\n",
      "    1      2012    55\n",
      "    4      2014    40\n",
      "    7      2013    84\n",
      "    10     2014    31\n",
      "    \n",
      "    Create a MultiIndex using columns 'year' and 'month':\n",
      "    \n",
      "    >>> df.set_index(['year', 'month'])\n",
      "                sale\n",
      "    year  month\n",
      "    2012  1     55\n",
      "    2014  4     40\n",
      "    2013  7     84\n",
      "    2014  10    31\n",
      "    \n",
      "    Create a MultiIndex using an Index and a column:\n",
      "    \n",
      "    >>> df.set_index([pd.Index([1, 2, 3, 4]), 'year'])\n",
      "             month  sale\n",
      "       year\n",
      "    1  2012  1      55\n",
      "    2  2014  4      40\n",
      "    3  2013  7      84\n",
      "    4  2014  10     31\n",
      "    \n",
      "    Create a MultiIndex using two Series:\n",
      "    \n",
      "    >>> s = pd.Series([1, 2, 3, 4])\n",
      "    >>> df.set_index([s, s**2])\n",
      "          month  year  sale\n",
      "    1 1       1  2012    55\n",
      "    2 4       4  2014    40\n",
      "    3 9       7  2013    84\n",
      "    4 16     10  2014    31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.loc[] - for more info, see https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#indexing-label\n",
    "\n",
    "print(df.index) # the default index when reading in a file is a range index. In this case,\n",
    "                 # .loc and .iloc works ALMOST the same.\n",
    "# one difference:\n",
    "#print(df.loc[3:9:2]) # this selects the 4th, 6th, 8th, 10th rows - the stop element is included!\n",
    "\n",
    "help(df.set_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     age     workclass  fnlwgt      education  education-num  \\\n",
      "age                                                            \n",
      "30    30     State-gov  141297      Bachelors             13   \n",
      "30    30   Federal-gov   59951   Some-college             10   \n",
      "30    30       Private  188146        HS-grad              9   \n",
      "30    30       Private   59496      Bachelors             13   \n",
      "30    30       Private   54334            9th              5   \n",
      "\n",
      "          marital-status          occupation    relationship  \\\n",
      "age                                                            \n",
      "30    Married-civ-spouse      Prof-specialty         Husband   \n",
      "30    Married-civ-spouse        Adm-clerical       Own-child   \n",
      "30    Married-civ-spouse   Machine-op-inspct         Husband   \n",
      "30    Married-civ-spouse               Sales         Husband   \n",
      "30         Never-married               Sales   Not-in-family   \n",
      "\n",
      "                    race    sex  capital-gain  capital-loss  hours-per-week  \\\n",
      "age                                                                           \n",
      "30    Asian-Pac-Islander   Male             0             0              40   \n",
      "30                 White   Male             0             0              40   \n",
      "30                 White   Male          5013             0              40   \n",
      "30                 White   Male          2407             0              40   \n",
      "30                 White   Male             0             0              40   \n",
      "\n",
      "     native-country gross-income  \n",
      "age                               \n",
      "30            India         >50K  \n",
      "30    United-States        <=50K  \n",
      "30    United-States        <=50K  \n",
      "30    United-States        <=50K  \n",
      "30    United-States        <=50K  \n"
     ]
    }
   ],
   "source": [
    "df_index_age = df.set_index('age',drop=False)\n",
    "\n",
    "#print(df_index_age.index)\n",
    "#print(df_index_age.head())\n",
    "\n",
    "print(df_index_age.loc[30].head()) # collect everyone with age 30 - the index is non-unique\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3) select rows based on column condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       age          workclass  fnlwgt      education  education-num  \\\n",
      "222     90            Private   51744        HS-grad              9   \n",
      "1040    90            Private  137018        HS-grad              9   \n",
      "1935    90            Private  221832      Bachelors             13   \n",
      "2303    90            Private   52386   Some-college             10   \n",
      "2891    90            Private  171956   Some-college             10   \n",
      "4070    90            Private  313986           11th              7   \n",
      "4109    90                  ?  256514      Bachelors             13   \n",
      "5104    90            Private   52386   Some-college             10   \n",
      "5272    90            Private  141758            9th              5   \n",
      "5370    90          Local-gov  227796        Masters             14   \n",
      "5406    90            Private   51744        Masters             14   \n",
      "6232    90   Self-emp-not-inc  155981      Bachelors             13   \n",
      "6624    90            Private  313986           11th              7   \n",
      "8562    49            Private  122066        HS-grad              9   \n",
      "8806    90            Private   87372    Prof-school             15   \n",
      "8963    90                  ?   77053        HS-grad              9   \n",
      "8973    90            Private   46786      Bachelors             13   \n",
      "10210   90   Self-emp-not-inc  282095   Some-college             10   \n",
      "10545   90            Private  175491        HS-grad              9   \n",
      "11512   90            Private   87285        HS-grad              9   \n",
      "11731   90                  ?   39824        HS-grad              9   \n",
      "11996   90            Private   40388      Bachelors             13   \n",
      "12451   90                  ?  225063   Some-college             10   \n",
      "12529   65            Private  172510   Some-college             10   \n",
      "12975   90            Private  250832           10th              6   \n",
      "13928   81   Self-emp-not-inc  123959      Bachelors             13   \n",
      "14159   90          Local-gov  187749     Assoc-acdm             12   \n",
      "15259   60            Private  114263      Bachelors             13   \n",
      "15356   90            Private   90523        HS-grad              9   \n",
      "15892   90            Private   88991      Bachelors             13   \n",
      "17144   28   Self-emp-not-inc  183523        HS-grad              9   \n",
      "17735   26            Private  358975   Some-college             10   \n",
      "18277   90            Private  311184      Bachelors             13   \n",
      "18413   90            Private  313749      Bachelors             13   \n",
      "18725   90          Local-gov  153602        HS-grad              9   \n",
      "18832   90            Private  115306        Masters             14   \n",
      "18839   66   Self-emp-not-inc  174995     Assoc-acdm             12   \n",
      "19212   90            Private  139660   Some-college             10   \n",
      "19489   90            Private   84553        HS-grad              9   \n",
      "19747   90            Private  226968        7th-8th              4   \n",
      "20610   90            Private  206667        Masters             14   \n",
      "21371   30            Private  207668      Bachelors             13   \n",
      "22220   90            Private   52386      Bachelors             13   \n",
      "22658   54            Private  188186        HS-grad              9   \n",
      "23023   24            Private  117779      Bachelors             13   \n",
      "24043   90   Self-emp-not-inc   82628        HS-grad              9   \n",
      "24238   90                  ?  166343        1st-4th              2   \n",
      "25303   90                  ?  175444        7th-8th              4   \n",
      "27041   57       Self-emp-inc  258883        HS-grad              9   \n",
      "27750   55            Private  143266      Assoc-voc             11   \n",
      "28463   90        Federal-gov  195433        HS-grad              9   \n",
      "30346   47            Private  180277        HS-grad              9   \n",
      "31030   90            Private   47929        HS-grad              9   \n",
      "31696   90                  ?  313986        HS-grad              9   \n",
      "32277   90            Private  313749        HS-grad              9   \n",
      "32367   90          Local-gov  214594        7th-8th              4   \n",
      "\n",
      "            marital-status          occupation     relationship  \\\n",
      "222          Never-married       Other-service    Not-in-family   \n",
      "1040         Never-married       Other-service    Not-in-family   \n",
      "1935    Married-civ-spouse     Exec-managerial          Husband   \n",
      "2303         Never-married       Other-service    Not-in-family   \n",
      "2891             Separated        Adm-clerical        Own-child   \n",
      "4070         Never-married   Handlers-cleaners        Own-child   \n",
      "4109               Widowed                   ?   Other-relative   \n",
      "5104         Never-married       Other-service    Not-in-family   \n",
      "5272         Never-married        Adm-clerical    Not-in-family   \n",
      "5370    Married-civ-spouse     Exec-managerial          Husband   \n",
      "5406         Never-married     Exec-managerial    Not-in-family   \n",
      "6232    Married-civ-spouse      Prof-specialty          Husband   \n",
      "6624    Married-civ-spouse        Craft-repair          Husband   \n",
      "8562    Married-civ-spouse     Exec-managerial          Husband   \n",
      "8806    Married-civ-spouse      Prof-specialty          Husband   \n",
      "8963               Widowed                   ?    Not-in-family   \n",
      "8973    Married-civ-spouse               Sales          Husband   \n",
      "10210   Married-civ-spouse     Farming-fishing          Husband   \n",
      "10545   Married-civ-spouse        Craft-repair          Husband   \n",
      "11512        Never-married       Other-service        Own-child   \n",
      "11731              Widowed                   ?    Not-in-family   \n",
      "11996        Never-married     Exec-managerial    Not-in-family   \n",
      "12451        Never-married                   ?        Own-child   \n",
      "12529              Widowed      Prof-specialty    Not-in-family   \n",
      "12975   Married-civ-spouse     Exec-managerial          Husband   \n",
      "13928              Widowed      Prof-specialty    Not-in-family   \n",
      "14159   Married-civ-spouse        Adm-clerical          Husband   \n",
      "15259             Divorced     Exec-managerial    Not-in-family   \n",
      "15356              Widowed    Transport-moving        Unmarried   \n",
      "15892   Married-civ-spouse     Exec-managerial             Wife   \n",
      "17144   Married-civ-spouse        Craft-repair          Husband   \n",
      "17735        Never-married     Priv-house-serv    Not-in-family   \n",
      "18277   Married-civ-spouse               Sales          Husband   \n",
      "18413        Never-married      Prof-specialty        Own-child   \n",
      "18725   Married-civ-spouse       Other-service          Husband   \n",
      "18832        Never-married     Exec-managerial        Own-child   \n",
      "18839   Married-civ-spouse        Craft-repair          Husband   \n",
      "19212             Divorced               Sales        Unmarried   \n",
      "19489   Married-civ-spouse   Machine-op-inspct          Husband   \n",
      "19747   Married-civ-spouse   Machine-op-inspct          Husband   \n",
      "20610   Married-civ-spouse      Prof-specialty             Wife   \n",
      "21371        Never-married        Tech-support    Not-in-family   \n",
      "22220        Never-married      Prof-specialty    Not-in-family   \n",
      "22658        Never-married       Other-service   Other-relative   \n",
      "23023        Never-married      Prof-specialty    Not-in-family   \n",
      "24043        Never-married     Exec-managerial    Not-in-family   \n",
      "24238              Widowed                   ?    Not-in-family   \n",
      "25303            Separated                   ?    Not-in-family   \n",
      "27041   Married-civ-spouse    Transport-moving          Husband   \n",
      "27750   Married-civ-spouse        Craft-repair          Husband   \n",
      "28463   Married-civ-spouse        Craft-repair          Husband   \n",
      "30346   Married-civ-spouse        Adm-clerical             Wife   \n",
      "31030   Married-civ-spouse   Machine-op-inspct          Husband   \n",
      "31696   Married-civ-spouse                   ?          Husband   \n",
      "32277              Widowed        Adm-clerical        Unmarried   \n",
      "32367   Married-civ-spouse     Protective-serv          Husband   \n",
      "\n",
      "                      race      sex  capital-gain  capital-loss  \\\n",
      "222                  Black     Male             0          2206   \n",
      "1040                 White   Female             0             0   \n",
      "1935                 White     Male             0             0   \n",
      "2303    Asian-Pac-Islander     Male             0             0   \n",
      "2891                 White   Female             0             0   \n",
      "4070                 White     Male             0             0   \n",
      "4109                 White   Female           991             0   \n",
      "5104    Asian-Pac-Islander     Male             0             0   \n",
      "5272                 White   Female             0             0   \n",
      "5370                 White     Male         20051             0   \n",
      "5406                 Black     Male             0             0   \n",
      "6232                 White     Male         10566             0   \n",
      "6624                 White     Male             0             0   \n",
      "8562                 White     Male             0             0   \n",
      "8806                 White     Male         20051             0   \n",
      "8963                 White   Female             0          4356   \n",
      "8973                 White     Male          9386             0   \n",
      "10210                White     Male             0             0   \n",
      "10545                White     Male          9386             0   \n",
      "11512                White   Female             0             0   \n",
      "11731                White     Male           401             0   \n",
      "11996                White     Male             0             0   \n",
      "12451   Asian-Pac-Islander     Male             0             0   \n",
      "12529                White   Female          1848             0   \n",
      "12975                White     Male             0             0   \n",
      "13928                White   Female             0          1668   \n",
      "14159   Asian-Pac-Islander     Male             0             0   \n",
      "15259                White   Female             0             0   \n",
      "15356                White     Male             0             0   \n",
      "15892                White   Female             0             0   \n",
      "17144                White     Male             0             0   \n",
      "17735                White   Female             0             0   \n",
      "18277                White     Male             0             0   \n",
      "18413                White   Female             0             0   \n",
      "18725                White     Male          6767             0   \n",
      "18832                White   Female             0             0   \n",
      "18839                White     Male          2290             0   \n",
      "19212                Black   Female             0             0   \n",
      "19489                White     Male             0             0   \n",
      "19747                White     Male             0             0   \n",
      "20610                White   Female             0             0   \n",
      "21371                White     Male             0             0   \n",
      "22220   Asian-Pac-Islander     Male             0             0   \n",
      "22658                White   Female             0             0   \n",
      "23023                White     Male             0             0   \n",
      "24043                White     Male          2964             0   \n",
      "24238                Black   Female             0             0   \n",
      "25303                White   Female             0             0   \n",
      "27041                White     Male          5178             0   \n",
      "27750                White     Male             0             0   \n",
      "28463                White     Male             0             0   \n",
      "30346                White   Female             0             0   \n",
      "31030                White     Male             0             0   \n",
      "31696                White     Male             0             0   \n",
      "32277                White   Female             0             0   \n",
      "32367                White     Male          2653             0   \n",
      "\n",
      "       hours-per-week  native-country gross-income  \n",
      "222                40   United-States        <=50K  \n",
      "1040               40   United-States        <=50K  \n",
      "1935               45   United-States        <=50K  \n",
      "2303               35   United-States        <=50K  \n",
      "2891               40     Puerto-Rico        <=50K  \n",
      "4070               40   United-States        <=50K  \n",
      "4109               10   United-States        <=50K  \n",
      "5104               35   United-States        <=50K  \n",
      "5272               40   United-States        <=50K  \n",
      "5370               60   United-States         >50K  \n",
      "5406               50   United-States         >50K  \n",
      "6232               50   United-States        <=50K  \n",
      "6624               40   United-States        <=50K  \n",
      "8562               30         Hungary        <=50K  \n",
      "8806               72   United-States         >50K  \n",
      "8963               40   United-States        <=50K  \n",
      "8973               15   United-States         >50K  \n",
      "10210              40   United-States        <=50K  \n",
      "10545              50         Ecuador         >50K  \n",
      "11512              24   United-States        <=50K  \n",
      "11731               4   United-States        <=50K  \n",
      "11996              55   United-States        <=50K  \n",
      "12451              10           South        <=50K  \n",
      "12529              20         Hungary        <=50K  \n",
      "12975              40   United-States        <=50K  \n",
      "13928               3         Hungary        <=50K  \n",
      "14159              20     Philippines        <=50K  \n",
      "15259              40         Hungary         >50K  \n",
      "15356              99   United-States        <=50K  \n",
      "15892              40         England         >50K  \n",
      "17144              50         Hungary        <=50K  \n",
      "17735              50         Hungary        <=50K  \n",
      "18277              20               ?        <=50K  \n",
      "18413              10   United-States        <=50K  \n",
      "18725              40   United-States        <=50K  \n",
      "18832              40   United-States        <=50K  \n",
      "18839              30         Hungary        <=50K  \n",
      "19212              37   United-States        <=50K  \n",
      "19489              40   United-States        <=50K  \n",
      "19747              40   United-States        <=50K  \n",
      "20610              40   United-States         >50K  \n",
      "21371              60         Hungary        <=50K  \n",
      "22220              40   United-States        <=50K  \n",
      "22658              20         Hungary        <=50K  \n",
      "23023              10         Hungary        <=50K  \n",
      "24043              12   United-States        <=50K  \n",
      "24238              40   United-States        <=50K  \n",
      "25303              15   United-States        <=50K  \n",
      "27041              60         Hungary         >50K  \n",
      "27750              50         Hungary         >50K  \n",
      "28463              30   United-States        <=50K  \n",
      "30346              40         Hungary        <=50K  \n",
      "31030              40   United-States        <=50K  \n",
      "31696              40   United-States         >50K  \n",
      "32277              25   United-States        <=50K  \n",
      "32367              40   United-States        <=50K  \n"
     ]
    }
   ],
   "source": [
    "# one condition\n",
    "#print(df[df['age']==30].head())\n",
    "# here is the condition: it's a boolean series - series is basically a dataframe with one column\n",
    "#print(df['age']==30)\n",
    "\n",
    "# multiple conditions can be combined with & (and) | (or)\n",
    "#print(df[(df['age']>30)&(df['age']<35)].head())\n",
    "print(df[(df['age']==90)|(df['native-country']==' Hungary')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Exercise 2\n",
    "How many people in adult_data.csv work at least 60 hours a week and have a doctorate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# <font color='LIGHTGRAY'><center> Data transformations: pandas data frames </center></font>\n",
    "###  <font color='LIGHTGRAY'>By the end of this lecture, you will be able to</font>\n",
    "   - <font color='LIGHTGRAY'>read in csv, excel, and sql data into a pandas data frame</font>\n",
    "   -  <font color='LIGHTGRAY'>filter rows in various ways</font>\n",
    "   -  **select columns**\n",
    "   -  <font color='LIGHTGRAY'>merge and append data frames</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       age  fnlwgt  education-num          occupation    race  capital-gain  \\\n",
      "0       39   77516             13        Adm-clerical   White          2174   \n",
      "1       50   83311             13     Exec-managerial   White             0   \n",
      "2       38  215646              9   Handlers-cleaners   White             0   \n",
      "3       53  234721              7   Handlers-cleaners   Black             0   \n",
      "4       28  338409             13      Prof-specialty   Black             0   \n",
      "...    ...     ...            ...                 ...     ...           ...   \n",
      "32556   27  257302             12        Tech-support   White             0   \n",
      "32557   40  154374              9   Machine-op-inspct   White             0   \n",
      "32558   58  151910              9        Adm-clerical   White             0   \n",
      "32559   22  201490              9        Adm-clerical   White             0   \n",
      "32560   52  287927              9     Exec-managerial   White         15024   \n",
      "\n",
      "       hours-per-week gross-income  \n",
      "0                  40        <=50K  \n",
      "1                  13        <=50K  \n",
      "2                  40        <=50K  \n",
      "3                  40        <=50K  \n",
      "4                  40        <=50K  \n",
      "...               ...          ...  \n",
      "32556              38        <=50K  \n",
      "32557              40         >50K  \n",
      "32558              40        <=50K  \n",
      "32559              20        <=50K  \n",
      "32560              40         >50K  \n",
      "\n",
      "[32561 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "columns =  df.columns\n",
    "#print(columns)\n",
    "\n",
    "# select columns by column name\n",
    "#print(df[['age','hours-per-week']])\n",
    "#print(columns[[1,5,7]])\n",
    "#print(df[columns[[1,5,7]]])\n",
    "\n",
    "# select columns by index using iloc\n",
    "#print(df.iloc[:,3])\n",
    "\n",
    "# select columns by index - not standard python indexing\n",
    "#print(df.iloc[:,[3,5,6]])\n",
    "\n",
    "# select columns by index -  standard python indexing\n",
    "print(df.iloc[:,::2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <font color='LIGHTGRAY'><center> Data transformations: pandas data frames </center></font>\n",
    "###  <font color='LIGHTGRAY'>By the end of this lecture, you will be able to</font>\n",
    "   - <font color='LIGHTGRAY'>read in csv, excel, and sql data into a pandas data frame</font>\n",
    "   -  <font color='LIGHTGRAY'>filter rows in various ways</font>\n",
    "   -  <font color='LIGHTGRAY'>select columns</font>\n",
    "   -  **merge and append data frames**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How to merge dataframes?\n",
    "\n",
    "Merge - info on data points are distributed in multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID  col1 col2\n",
      "0  ID1     5    y\n",
      "1  ID2     8    j\n",
      "2  ID3     2    w\n",
      "3  ID4     6    b\n",
      "4  ID5     0    a\n",
      "5  ID6     2    b\n",
      "6  ID7     5    t\n",
      "     ID  col3 col2\n",
      "0   ID2    12    q\n",
      "1   ID5    76    u\n",
      "2   ID6    34    e\n",
      "3  ID10    98    l\n",
      "4  ID11    65    p\n"
     ]
    }
   ],
   "source": [
    "# We have two datasets from two hospitals\n",
    "\n",
    "hospital1 = {'ID':['ID1','ID2','ID3','ID4','ID5','ID6','ID7'],'col1':[5,8,2,6,0,2,5],'col2':['y','j','w','b','a','b','t']}\n",
    "df1 = pd.DataFrame(data=hospital1)\n",
    "print(df1)\n",
    "\n",
    "hospital2 = {'ID':['ID2','ID5','ID6','ID10','ID11'],'col3':[12,76,34,98,65],'col2':['q','u','e','l','p']}\n",
    "df2 = pd.DataFrame(data=hospital2)\n",
    "print(df2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# we are interested in only patients from hospital1\n",
    "#df_left = df1.merge(df2,how='left',on='ID') # IDs from the left dataframe (df1) are kept\n",
    "#print(df_left)\n",
    "\n",
    "# we are interested in only patients from hospital2\n",
    "#df_right = df1.merge(df2,how='right',on='ID') # IDs from the right dataframe (df2) are kept\n",
    "#print(df_right)\n",
    "\n",
    "# we are interested in patiens who were in both hospitals\n",
    "#df_inner = df1.merge(df2,how='inner',on='ID') # merging on IDs present in both dataframes\n",
    "#print(df_inner)\n",
    "\n",
    "# we are interested in all patients who visited at least one of the hospitals\n",
    "#df_outer = df1.merge(df2,how='outer',on='ID')  # merging on IDs present in any dataframe\n",
    "#print(df_outer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How to append dataframes?\n",
    "\n",
    "Append - new data comes in over a period of time. E.g., one file per month/quarter/fiscal year etc.\n",
    "\n",
    "\n",
    "You want to combine these files into one data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_append = df1.append(df2) # note that rows with ID2, ID5, and ID6  are duplicated! Indices are duplicated too.\n",
    "#print(df_append)\n",
    "\n",
    "#df_append = df1.append(df2,ignore_index=True) # note that rows with ID2, ID5, and ID6  are duplicated! \n",
    "#print(df_append)\n",
    "\n",
    "#d3 = {'ID':['ID23','ID94','ID56','ID17'],'col1':['rt','h','st','ne'],'col2':[23,86,23,78]}\n",
    "#df3 = pd.DataFrame(data=d3)\n",
    "#print(df3)\n",
    "\n",
    "#df_append = df1.append([df2,df3],ignore_index=True) # multiple dataframes can be appended to df1\n",
    "#print(df_append)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "raw_data_1 = {\n",
    "        'subject_id': ['1', '2', '3', '4', '5'],\n",
    "        'first_name': ['Alex', 'Amy', 'Allen', 'Alice', 'Ayoung'], \n",
    "        'last_name': ['Anderson', 'Ackerman', 'Ali', 'Aoni', 'Atiches']}\n",
    "\n",
    "raw_data_2 = {\n",
    "        'subject_id': ['6', '7', '8', '9', '10'],\n",
    "        'first_name': ['Billy', 'Brian', 'Bran', 'Bryce', 'Betty'], \n",
    "        'last_name': ['Bonder', 'Black', 'Balwner', 'Brice', 'Btisan']}\n",
    "\n",
    "raw_data_3 = {\n",
    "        'subject_id': ['1', '2', '3', '4', '5', '7', '8', '9', '10', '11'],\n",
    "        'test_id': [51, 15, 15, 61, 16, 14, 15, 1, 61, 16]}\n",
    "\n",
    "# Create three data frames from raw_data_1, 2, and 3.\n",
    "# Append the first two data frames and assign it to df_append.\n",
    "# Merge the third data frame with df_append such that only subject_ids from df_append are present. \n",
    "# Assign the new data frame to df_merge. \n",
    "# How many rows and columns do we have in df_merge?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Always check that the resulting dataframe is what you wanted to end up with!\n",
    "- small toy datasets are ideal to test your code.\n",
    "\n",
    "### If you need to do a more complicated dataframe operation, check out pd.concat()!\n",
    "\n",
    "### We will learn how to add/delete/modify columns later when we learn about feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### By now, you are able to\n",
    "   - read in csv, excel, and sql data into a pandas data frame\n",
    "   - filter rows in various ways\n",
    "   - select columns\n",
    "   - merge and append data frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mud card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
