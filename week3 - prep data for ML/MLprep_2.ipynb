{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mudcard\n",
    "- **Is there ever a reason to use stratification on a feature variable? Eg. If we have an indicator variable signifying whether market conditions are currently 'extreme'**\n",
    "    - usually you stratify on the target variable and not on a feature\n",
    "    - btw, we covered how to stratify on the classification target variable\n",
    "    - sometimes it is necessary to stratify on the regression target variable but sklearn has no function for it as far as I know\n",
    "        - if you ever encounter this problem, you'll need to write code for it\n",
    "- **For KFold splitting, how do we interpret the results from the k models? Do we somehow calculate an average of these models?**\n",
    "    - yes, if you have k folds, you'll calculate k training and validation scores for each model and hyperparameter combo\n",
    "    - you can calculate their means and stds to decide which model and hyperparameter combo is the best\n",
    "- **what is the disadvantage of the stratified method? Why we only applied it with imbalanced data?**\n",
    "    - there is no disadvantage as far as I know\n",
    "    - you can in principle apply stratified splitting in any classification problem\n",
    "    - but you MUST apply it if your problem is imbalanced\n",
    "- **why the cats and dogs on images are iid? Could you elaborate more about it ?**\n",
    "    - iid properties:\n",
    "        - independence: observing the current datapoint doesn't influence or provide insight about the next datapoint or any other datapoint in your sample\n",
    "            - the next cat or dog picture you download from the net is independent from all other pics you download\n",
    "        - identically distributed: all points are sampled from the same probability density distribution\n",
    "            - coin toss, or the IQ of people which follows a normal distribution\n",
    "            - no trends, the distribution doesn't change while you sample or collect the data\n",
    "    - time series: the next datapoint is influenced by the previous data point - autocorrelation\n",
    "    - group structure: each group could have a different probability density distribution\n",
    "- **I am not quite 100% understanding how is the K-fold works. I only see the validation and other data in the picture, but there is no training data. So how to split the other data in the K-fold approach?**\n",
    "    - in each split, the green folds are used for training and the blue fold is validation\n",
    "- **i think the difference between validation and testing is still vague. I guess I want to better understand (through an example) how validation is used in tuning.**\n",
    "    - I know, this material is difficult to teach and learn if you are new to ML\n",
    "    - I promise that everything will fall into place by early to mid November once you see the final product, an ML pipeline\n",
    "- **is it in the EDA process that we can decide which features are time-series structured and which features are group structured?**\n",
    "    - usually you only need to decide whether the target variable is time-series or group-structured\n",
    "- **If there are extremely rare cases, 2 out of 100000, then how should I put these cases into train/validation/test sets?**\n",
    "    - if your whole dataset contains only two samples of one class, I'd argue you don't have enough data to do proper ML\n",
    "    - you need to collect more data\n",
    "- **After using KFolds to split your train and validation sets, how should you proceed? Should you create a list for each to store the different splits and iterate through those in the next steps of the pipeline?**\n",
    "    - we iterated through the various splits in a for loop and we will just keep adding code to that for loop to cover the next steps\n",
    "- **What's the difference between ¬†train_test_split(X,y,train_size = 0.6,stratify=... and StratifiedShuffleSplit()?**\n",
    "    - train_test_split splits the data once\n",
    "        - the split is stratified if you use the stratify argument\n",
    "    - stratified shuffle split creates `n_splits` number of splits that are stratified\n",
    "- **are there best practices for picking random seeds/states?**\n",
    "    - nope :)\n",
    "    - you can literally use whatever number you want\n",
    "- **Why did you split the data in a different order when showing the code for how to complete Quiz 1?**\n",
    "    - the basic split can be performed in three different ways and all three ways are equally good\n",
    "    - split X to X_other, X_train, and then split X_other to X_val and X_test\n",
    "    - split X to X_other, X_val, and then split X_other to X_train and X_test\n",
    "    - split X to X_other, X_test, and then split X_other to X_train and X_val\n",
    "- **The concept of stratifying data set!**\n",
    "- **Could you please explain more about stratify=y in part two today?**\n",
    "    - read more [here](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation) \n",
    "- **is there any special case when not using shuffle is more meaningful?**\n",
    "    - not that I aware of\n",
    "    - you can use shuffle in any splitting strategy\n",
    "    - if your dataset is not shuffled, you MUST use shuffle\n",
    "- **For train_test_split, we had to specifiy which variable to stratify on. Does StratifiedKFold just assume that the second variable you pass is the one upon which to stratify?**\n",
    "    - yes, that's exactly right\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html\n",
    "- **I am still confused about what does the Class and Group in the image. Since it is an iid datasets, why is there Class and Group?**\n",
    "    - class is there because we illustrate the stratification and splits on a classification example\n",
    "    - you are right, group is not necessary when discussing an iid dataset \n",
    "    - we cover groups today\n",
    "- **Do you have any additional materials we might be able to review to complement this course?\n",
    "    - the sklearn website is a great resource\n",
    "    - check out the [model selection](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection) section\n",
    "- **What are the pros and cons of using kfold vs regular splitting for iid data?**\n",
    "    - a good way to approach this is to consider the sources of uncertainty on the generalization error\n",
    "    - source 1: the points in the test set\n",
    "        - if you have different points in the test set, the generalization error will be different for the same model\n",
    "    - source 2: the models are different\n",
    "        - if the points in the test set are the same, different models (and sometimes the same model trained with a different random seed) will give a different generalization error\n",
    "    - if you perform the basic split a couple of times with a couple of different seed, both sources of uncertainty will be measured in the generalization error\n",
    "    - if you do a kfold split, the points in the test set will be the same so you'll only measure source 2\n",
    "- **I've heard leave-one-out cross validation can lead to poor estimates of generalization error. Do you agree or is it generally better to use as many folds as your computational resources allow.**\n",
    "    - leave-one-out is pretty rarely used in general\n",
    "    - you'd only use that on small datasets\n",
    "    - but then you should consider not using ML anyway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Data preprocessing</center>\n",
    "### By the end of this lecture, you will be able to\n",
    "- apply one-hot encoding or ordinal encoding to categorical variables\n",
    "- apply scaling and normalization to continuous variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The supervised ML pipeline\n",
    "The goal: Use the training data (X and y) to develop a <font color='red'>model</font> which can <font color='red'>accurately</font> predict the target variable (y_new') for previously unseen data (X_new).\n",
    "\n",
    "**1. Exploratory Data Analysis (EDA)**: you need to understand your data and verify that it doesn't contain errors\n",
    "   - do as much EDA as you can!\n",
    "    \n",
    "**2. Split the data into different sets**: most often the sets are train, validation, and test (or holdout)\n",
    "   - practitioners often make errors in this step!\n",
    "   - you can split the data randomly, based on groups, based on time, or any other non-standard way if necessary to answer your ML question\n",
    "\n",
    "<span style=\"background-color: #FFFF00\">**3. Preprocess the data**: ML models only work if X and Y are numbers! Some ML models additionally require each feature to have 0 mean and 1 standard deviation (standardized features)</span>\n",
    "   - often the original features you get contain strings (for example a gender feature would contain 'male', 'female', 'non-binary', 'unknown') which needs to transformed into numbers\n",
    "   - often the features are not standardized (e.g., age is between 0 and 100) but it needs to be standardized\n",
    "    \n",
    "**4. Choose an evaluation metric**: depends on the priorities of the stakeholders\n",
    "   - often requires quite a bit of thinking and ethical considerations\n",
    "     \n",
    "**5. Choose one or more ML techniques**: it is highly recommended that you try multiple models\n",
    "   - start with simple models like linear or logistic regression\n",
    "   - try also more complex models like nearest neighbors, support vector machines, random forest, etc.\n",
    "    \n",
    "**6. Tune the hyperparameters of your ML models (aka cross-validation)**\n",
    "   - ML techniques have hyperparameters that you need to optimize to achieve best performance\n",
    "   - for each ML model, decide which parameters to tune and what values to try\n",
    "   - loop through each parameter combination\n",
    "       - train one model for each parameter combination\n",
    "       - evaluate how well the model performs on the validation set\n",
    "   - take the parameter combo that gives the best validation score\n",
    "   - evaluate that model on the test set to report how well the model is expected to perform on previously unseen data\n",
    "    \n",
    "**7. Interpret your model**: black boxes are often not useful\n",
    "   - check if your model uses features that make sense (excellent tool for debugging)\n",
    "   - often model predictions are not enough, you need to be able to explain how the model arrived to a particular prediction (e.g., in health care)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem description, why preprocessing is necessary\n",
    "\n",
    "Data format suitable for ML: 2D numerical values.\n",
    "\n",
    "| X|feature_1|feature_2|...|feature_j|...|feature_m|<font color='red'>y</font>|\n",
    "|-|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|__data_point_1__|x_11|x_12|...|x_1j|...|x_1m|__<font color='red'>y_1</font>__|\n",
    "|__data_point_2__|x_21|x_22|...|x_2j|...|x_2m|__<font color='red'>y_2</font>__|\n",
    "|__...__|...|...|...|...|...|...|__<font color='red'>...</font>__|\n",
    "|__data_point_i__|x_i1|x_i2|...|x_ij|...|x_im|__<font color='red'>y_i</font>__|\n",
    "|__...__|...|...|...|...|...|...|__<font color='red'>...</font>__|\n",
    "|__data_point_n__|x_n1|x_n2|...|x_nj|...|x_nm|__<font color='red'>y_n</font>__|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data almost never comes in a format that's directly usable in ML.\n",
    "- let's check the adult data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set\n",
      "       age   workclass  fnlwgt      education  education-num  \\\n",
      "25823   31     Private   87418      Assoc-voc             11   \n",
      "10274   41     Private  121718   Some-college             10   \n",
      "27652   61     Private   79827        HS-grad              9   \n",
      "13941   33   State-gov  156015      Bachelors             13   \n",
      "31384   38     Private  167882   Some-college             10   \n",
      "\n",
      "            marital-status        occupation     relationship    race  \\\n",
      "25823   Married-civ-spouse   Exec-managerial          Husband   White   \n",
      "10274   Married-civ-spouse      Craft-repair          Husband   White   \n",
      "27652   Married-civ-spouse   Exec-managerial          Husband   White   \n",
      "13941   Married-civ-spouse   Exec-managerial          Husband   White   \n",
      "31384              Widowed     Other-service   Other-relative   Black   \n",
      "\n",
      "           sex  capital-gain  capital-loss  hours-per-week  native-country  \n",
      "25823     Male             0             0              40   United-States  \n",
      "10274     Male             0             0              40           Italy  \n",
      "27652     Male             0             0              50   United-States  \n",
      "13941     Male             0             0              40   United-States  \n",
      "31384   Female             0             0              45           Haiti  \n",
      "25823     <=50K\n",
      "10274     <=50K\n",
      "27652     <=50K\n",
      "13941      >50K\n",
      "31384     <=50K\n",
      "Name: gross-income, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "df = pd.read_csv('data/adult_data.csv')\n",
    "\n",
    "# let's separate the feature matrix X, and target variable y\n",
    "y = df['gross-income'] # remember, we want to predict who earns more than 50k or less than 50k\n",
    "X = df.loc[:, df.columns != 'gross-income'] # all other columns are features\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "# first split to separate out the training set\n",
    "X_train, X_other, y_train, y_other = train_test_split(X,y,train_size = 0.6,random_state=random_state)\n",
    "\n",
    "# second split to separate out the validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_other,y_other,train_size = 0.5,random_state=random_state)\n",
    "\n",
    "print('training set')\n",
    "print(X_train.head()) # lots of strings!\n",
    "print(y_train.head()) # even our labels are strings and not numbers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### scikit-learn transformers to the rescue!\n",
    "\n",
    "Preprocessing is done with various transformers. All transformes have three methods:\n",
    "- **fit** method: estimates parameters necessary to do the transformation,\n",
    "- **transform** method: transforms the data based on the estimated parameters,\n",
    "- **fit_transform** method: both steps are performed at once, this can be faster than doing the steps separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transformers we cover today\n",
    "- **OneHotEncoder** - converts categorical features into dummy arrays\n",
    "- **OrdinalEncoder** - converts categorical features into an integer array\n",
    "- **MinMaxScaler** - scales continuous variables to be between 0 and 1\n",
    "- **StandardScaler** - standardizes continuous features by removing the mean and scaling to unit variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='LIGHTGRAY'>By the end of this lecture, you will be able to</font>\n",
    "- **apply one-hot encoding or ordinal encoding to categorical variables**\n",
    "- <font color='LIGHTGRAY'>apply scaling and normalization to continuous variables</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ordered categorical data: OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- use it on categorical features if the categories can be ranked or ordered\n",
    "    - educational level in the adult dataset\n",
    "    - reaction to medication is described by words like 'severe', 'no response', 'excellent'\n",
    "    - any time you know that the categories can be clearly ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class OrdinalEncoder in module sklearn.preprocessing._encoders:\n",
      "\n",
      "class OrdinalEncoder(_BaseEncoder)\n",
      " |  OrdinalEncoder(*, categories='auto', dtype=<class 'numpy.float64'>, handle_unknown='error', unknown_value=None)\n",
      " |  \n",
      " |  Encode categorical features as an integer array.\n",
      " |  \n",
      " |  The input to this transformer should be an array-like of integers or\n",
      " |  strings, denoting the values taken on by categorical (discrete) features.\n",
      " |  The features are converted to ordinal integers. This results in\n",
      " |  a single column of integers (0 to n_categories - 1) per feature.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n",
      " |  \n",
      " |  .. versionadded:: 0.20\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  categories : 'auto' or a list of array-like, default='auto'\n",
      " |      Categories (unique values) per feature:\n",
      " |  \n",
      " |      - 'auto' : Determine categories automatically from the training data.\n",
      " |      - list : ``categories[i]`` holds the categories expected in the ith\n",
      " |        column. The passed categories should not mix strings and numeric\n",
      " |        values, and should be sorted in case of numeric values.\n",
      " |  \n",
      " |      The used categories can be found in the ``categories_`` attribute.\n",
      " |  \n",
      " |  dtype : number type, default np.float64\n",
      " |      Desired dtype of output.\n",
      " |  \n",
      " |  handle_unknown : {'error', 'use_encoded_value'}, default='error'\n",
      " |      When set to 'error' an error will be raised in case an unknown\n",
      " |      categorical feature is present during transform. When set to\n",
      " |      'use_encoded_value', the encoded value of unknown categories will be\n",
      " |      set to the value given for the parameter `unknown_value`. In\n",
      " |      :meth:`inverse_transform`, an unknown category will be denoted as None.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  unknown_value : int or np.nan, default=None\n",
      " |      When the parameter handle_unknown is set to 'use_encoded_value', this\n",
      " |      parameter is required and will set the encoded value of unknown\n",
      " |      categories. It has to be distinct from the values used to encode any of\n",
      " |      the categories in `fit`. If set to np.nan, the `dtype` parameter must\n",
      " |      be a float dtype.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  categories_ : list of arrays\n",
      " |      The categories of each feature determined during ``fit`` (in order of\n",
      " |      the features in X and corresponding with the output of ``transform``).\n",
      " |      This does not include categories that weren't seen during ``fit``.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  OneHotEncoder : Performs a one-hot encoding of categorical features.\n",
      " |  LabelEncoder : Encodes target labels with values between 0 and\n",
      " |      ``n_classes-1``.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  Given a dataset with two features, we let the encoder find the unique\n",
      " |  values per feature and transform the data to an ordinal encoding.\n",
      " |  \n",
      " |  >>> from sklearn.preprocessing import OrdinalEncoder\n",
      " |  >>> enc = OrdinalEncoder()\n",
      " |  >>> X = [['Male', 1], ['Female', 3], ['Female', 2]]\n",
      " |  >>> enc.fit(X)\n",
      " |  OrdinalEncoder()\n",
      " |  >>> enc.categories_\n",
      " |  [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n",
      " |  >>> enc.transform([['Female', 3], ['Male', 1]])\n",
      " |  array([[0., 2.],\n",
      " |         [1., 0.]])\n",
      " |  \n",
      " |  >>> enc.inverse_transform([[1, 0], [0, 1]])\n",
      " |  array([['Male', 1],\n",
      " |         ['Female', 2]], dtype=object)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      OrdinalEncoder\n",
      " |      _BaseEncoder\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, categories='auto', dtype=<class 'numpy.float64'>, handle_unknown='error', unknown_value=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Fit the OrdinalEncoder to X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The data to determine the categories of each feature.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored. This parameter exists only for compatibility with\n",
      " |          :class:`~sklearn.pipeline.Pipeline`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Convert the data back to the original representation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The transformed data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_tr : ndarray of shape (n_samples, n_features)\n",
      " |          Inverse transformed array.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Transform X to ordinal codes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The data to encode.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_out : ndarray of shape (n_samples, n_features)\n",
      " |          Transformed input.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      " |      and returns a transformed version of `X`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input samples.\n",
      " |      \n",
      " |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      " |          Target values (None for unsupervised transformations).\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "help(OrdinalEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['HS-grad', 'College', 'Bachelors', 'Masters', 'Doctorate'],\n",
      "      dtype=object)]\n",
      "[[2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [0.]\n",
      " [3.]]\n",
      "[[0.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]]\n"
     ]
    }
   ],
   "source": [
    "# toy example\n",
    "import pandas as pd\n",
    "\n",
    "train_edu = {'educational level':['Bachelors','Masters','Bachelors','Doctorate','HS-grad','Masters']} \n",
    "test_edu = {'educational level':['HS-grad','Masters','Masters','College','Bachelors']}\n",
    "\n",
    "Xtoy_train = pd.DataFrame(train_edu)\n",
    "Xtoy_test = pd.DataFrame(test_edu)\n",
    "\n",
    "# initialize the encoder\n",
    "cats = [['HS-grad','College','Bachelors','Masters','Doctorate']]\n",
    "\n",
    "enc = OrdinalEncoder(categories = cats) # The ordered list of \n",
    "# categories need to be provided. By default, the categories are alphabetically ordered!\n",
    "\n",
    "# fit the training data\n",
    "enc.fit(Xtoy_train)\n",
    "# print the categories - not really important because we manually gave the ordered list of categories\n",
    "print(enc.categories_)\n",
    "# transform X_train. We could have used enc.fit_transform(X_train) to combine fit and transform\n",
    "X_train_oe = enc.transform(Xtoy_train)\n",
    "print(X_train_oe)\n",
    "# transform X_test\n",
    "X_test_oe = enc.transform(Xtoy_test) # OrdinalEncoder always throws an error message if \n",
    "                                  # it encounters an unknown category in test\n",
    "print(X_test_oe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformed train features:\n",
      "[[10.]\n",
      " [ 9.]\n",
      " [ 8.]\n",
      " ...\n",
      " [ 6.]\n",
      " [ 8.]\n",
      " [12.]]\n",
      "transformed validation features:\n",
      "[[14.]\n",
      " [13.]\n",
      " [ 9.]\n",
      " ...\n",
      " [12.]\n",
      " [ 8.]\n",
      " [ 8.]]\n",
      "transformed test features:\n",
      "[[12.]\n",
      " [ 9.]\n",
      " [12.]\n",
      " ...\n",
      " [ 9.]\n",
      " [ 9.]\n",
      " [11.]]\n"
     ]
    }
   ],
   "source": [
    "# apply OE to the adult dataset\n",
    "# initialize the encoder\n",
    "ordinal_ftrs = ['education'] # if you have more than one ordinal feature, add the feature names here\n",
    "ordinal_cats = [[' Preschool',' 1st-4th',' 5th-6th',' 7th-8th',' 9th',' 10th',' 11th',' 12th',' HS-grad',\\\n",
    "                ' Some-college',' Assoc-voc',' Assoc-acdm',' Bachelors',' Masters',' Prof-school',' Doctorate']]\n",
    "# ordinal_cats must contain one list per ordinal feature! each list contains the ordered list of categories \n",
    "# of the corresponding feature\n",
    "\n",
    "enc = OrdinalEncoder(categories = ordinal_cats)   # By default, the categories are alphabetically ordered\n",
    "                                                    # which is NOT what you want usually.\n",
    "\n",
    "# fit the training data\n",
    "enc.fit(X_train[ordinal_ftrs])  # the encoder expects a 2D array, that's why the column name is in a list\n",
    "\n",
    "# transform X_train. We could use enc.fit_transform(X_train) to combine fit and transform\n",
    "ordinal_train = enc.transform(X_train[ordinal_ftrs])\n",
    "print('transformed train features:')\n",
    "print(ordinal_train)\n",
    "# transform X_val\n",
    "ordinal_val = enc.transform(X_val[ordinal_ftrs])\n",
    "print('transformed validation features:')\n",
    "print(ordinal_val)\n",
    "# transform X_test\n",
    "ordinal_test = enc.transform(X_test[ordinal_ftrs])\n",
    "print('transformed test features:')\n",
    "print(ordinal_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unordered categorical data: one-hot encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- some categories cannot be ordered. e.g., workclass, relationship status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class OneHotEncoder in module sklearn.preprocessing._encoders:\n",
      "\n",
      "class OneHotEncoder(_BaseEncoder)\n",
      " |  OneHotEncoder(*, categories='auto', drop=None, sparse=True, dtype=<class 'numpy.float64'>, handle_unknown='error')\n",
      " |  \n",
      " |  Encode categorical features as a one-hot numeric array.\n",
      " |  \n",
      " |  The input to this transformer should be an array-like of integers or\n",
      " |  strings, denoting the values taken on by categorical (discrete) features.\n",
      " |  The features are encoded using a one-hot (aka 'one-of-K' or 'dummy')\n",
      " |  encoding scheme. This creates a binary column for each category and\n",
      " |  returns a sparse matrix or dense array (depending on the ``sparse``\n",
      " |  parameter)\n",
      " |  \n",
      " |  By default, the encoder derives the categories based on the unique values\n",
      " |  in each feature. Alternatively, you can also specify the `categories`\n",
      " |  manually.\n",
      " |  \n",
      " |  This encoding is needed for feeding categorical data to many scikit-learn\n",
      " |  estimators, notably linear models and SVMs with the standard kernels.\n",
      " |  \n",
      " |  Note: a one-hot encoding of y labels should use a LabelBinarizer\n",
      " |  instead.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  categories : 'auto' or a list of array-like, default='auto'\n",
      " |      Categories (unique values) per feature:\n",
      " |  \n",
      " |      - 'auto' : Determine categories automatically from the training data.\n",
      " |      - list : ``categories[i]`` holds the categories expected in the ith\n",
      " |        column. The passed categories should not mix strings and numeric\n",
      " |        values within a single feature, and should be sorted in case of\n",
      " |        numeric values.\n",
      " |  \n",
      " |      The used categories can be found in the ``categories_`` attribute.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  drop : {'first', 'if_binary'} or a array-like of shape (n_features,),             default=None\n",
      " |      Specifies a methodology to use to drop one of the categories per\n",
      " |      feature. This is useful in situations where perfectly collinear\n",
      " |      features cause problems, such as when feeding the resulting data\n",
      " |      into a neural network or an unregularized regression.\n",
      " |  \n",
      " |      However, dropping one category breaks the symmetry of the original\n",
      " |      representation and can therefore induce a bias in downstream models,\n",
      " |      for instance for penalized linear classification or regression models.\n",
      " |  \n",
      " |      - None : retain all features (the default).\n",
      " |      - 'first' : drop the first category in each feature. If only one\n",
      " |        category is present, the feature will be dropped entirely.\n",
      " |      - 'if_binary' : drop the first category in each feature with two\n",
      " |        categories. Features with 1 or more than 2 categories are\n",
      " |        left intact.\n",
      " |      - array : ``drop[i]`` is the category in feature ``X[:, i]`` that\n",
      " |        should be dropped.\n",
      " |  \n",
      " |      .. versionadded:: 0.21\n",
      " |         The parameter `drop` was added in 0.21.\n",
      " |  \n",
      " |      .. versionchanged:: 0.23\n",
      " |         The option `drop='if_binary'` was added in 0.23.\n",
      " |  \n",
      " |  sparse : bool, default=True\n",
      " |      Will return sparse matrix if set True else will return an array.\n",
      " |  \n",
      " |  dtype : number type, default=float\n",
      " |      Desired dtype of output.\n",
      " |  \n",
      " |  handle_unknown : {'error', 'ignore'}, default='error'\n",
      " |      Whether to raise an error or ignore if an unknown categorical feature\n",
      " |      is present during transform (default is to raise). When this parameter\n",
      " |      is set to 'ignore' and an unknown category is encountered during\n",
      " |      transform, the resulting one-hot encoded columns for this feature\n",
      " |      will be all zeros. In the inverse transform, an unknown category\n",
      " |      will be denoted as None.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  categories_ : list of arrays\n",
      " |      The categories of each feature determined during fitting\n",
      " |      (in order of the features in X and corresponding with the output\n",
      " |      of ``transform``). This includes the category specified in ``drop``\n",
      " |      (if any).\n",
      " |  \n",
      " |  drop_idx_ : array of shape (n_features,)\n",
      " |      - ``drop_idx_[i]`` is the index in ``categories_[i]`` of the category\n",
      " |        to be dropped for each feature.\n",
      " |      - ``drop_idx_[i] = None`` if no category is to be dropped from the\n",
      " |        feature with index ``i``, e.g. when `drop='if_binary'` and the\n",
      " |        feature isn't binary.\n",
      " |      - ``drop_idx_ = None`` if all the transformed features will be\n",
      " |        retained.\n",
      " |  \n",
      " |      .. versionchanged:: 0.23\n",
      " |         Added the possibility to contain `None` values.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  OrdinalEncoder : Performs an ordinal (integer)\n",
      " |    encoding of the categorical features.\n",
      " |  sklearn.feature_extraction.DictVectorizer : Performs a one-hot encoding of\n",
      " |    dictionary items (also handles string-valued features).\n",
      " |  sklearn.feature_extraction.FeatureHasher : Performs an approximate one-hot\n",
      " |    encoding of dictionary items or strings.\n",
      " |  LabelBinarizer : Binarizes labels in a one-vs-all\n",
      " |    fashion.\n",
      " |  MultiLabelBinarizer : Transforms between iterable of\n",
      " |    iterables and a multilabel format, e.g. a (samples x classes) binary\n",
      " |    matrix indicating the presence of a class label.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  Given a dataset with two features, we let the encoder find the unique\n",
      " |  values per feature and transform the data to a binary one-hot encoding.\n",
      " |  \n",
      " |  >>> from sklearn.preprocessing import OneHotEncoder\n",
      " |  \n",
      " |  One can discard categories not seen during `fit`:\n",
      " |  \n",
      " |  >>> enc = OneHotEncoder(handle_unknown='ignore')\n",
      " |  >>> X = [['Male', 1], ['Female', 3], ['Female', 2]]\n",
      " |  >>> enc.fit(X)\n",
      " |  OneHotEncoder(handle_unknown='ignore')\n",
      " |  >>> enc.categories_\n",
      " |  [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n",
      " |  >>> enc.transform([['Female', 1], ['Male', 4]]).toarray()\n",
      " |  array([[1., 0., 1., 0., 0.],\n",
      " |         [0., 1., 0., 0., 0.]])\n",
      " |  >>> enc.inverse_transform([[0, 1, 1, 0, 0], [0, 0, 0, 1, 0]])\n",
      " |  array([['Male', 1],\n",
      " |         [None, 2]], dtype=object)\n",
      " |  >>> enc.get_feature_names(['gender', 'group'])\n",
      " |  array(['gender_Female', 'gender_Male', 'group_1', 'group_2', 'group_3'],\n",
      " |    dtype=object)\n",
      " |  \n",
      " |  One can always drop the first column for each feature:\n",
      " |  \n",
      " |  >>> drop_enc = OneHotEncoder(drop='first').fit(X)\n",
      " |  >>> drop_enc.categories_\n",
      " |  [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n",
      " |  >>> drop_enc.transform([['Female', 1], ['Male', 2]]).toarray()\n",
      " |  array([[0., 0., 0.],\n",
      " |         [1., 1., 0.]])\n",
      " |  \n",
      " |  Or drop a column for feature only having 2 categories:\n",
      " |  \n",
      " |  >>> drop_binary_enc = OneHotEncoder(drop='if_binary').fit(X)\n",
      " |  >>> drop_binary_enc.transform([['Female', 1], ['Male', 2]]).toarray()\n",
      " |  array([[0., 1., 0., 0.],\n",
      " |         [1., 0., 1., 0.]])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      OneHotEncoder\n",
      " |      _BaseEncoder\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, categories='auto', drop=None, sparse=True, dtype=<class 'numpy.float64'>, handle_unknown='error')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Fit OneHotEncoder to X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The data to determine the categories of each feature.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored. This parameter exists only for compatibility with\n",
      " |          :class:`~sklearn.pipeline.Pipeline`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  fit_transform(self, X, y=None)\n",
      " |      Fit OneHotEncoder to X, then transform X.\n",
      " |      \n",
      " |      Equivalent to fit(X).transform(X) but more convenient.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The data to encode.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored. This parameter exists only for compatibility with\n",
      " |          :class:`~sklearn.pipeline.Pipeline`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_out : {ndarray, sparse matrix} of shape                 (n_samples, n_encoded_features)\n",
      " |          Transformed input. If `sparse=True`, a sparse matrix will be\n",
      " |          returned.\n",
      " |  \n",
      " |  get_feature_names(self, input_features=None)\n",
      " |      Return feature names for output features.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      input_features : list of str of shape (n_features,)\n",
      " |          String names for input features if available. By default,\n",
      " |          \"x0\", \"x1\", ... \"xn_features\" is used.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      output_feature_names : ndarray of shape (n_output_features,)\n",
      " |          Array of feature names.\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Convert the data back to the original representation.\n",
      " |      \n",
      " |      In case unknown categories are encountered (all zeros in the\n",
      " |      one-hot encoding), ``None`` is used to represent this category.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape                 (n_samples, n_encoded_features)\n",
      " |          The transformed data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_tr : ndarray of shape (n_samples, n_features)\n",
      " |          Inverse transformed array.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Transform X using one-hot encoding.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The data to encode.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_out : {ndarray, sparse matrix} of shape                 (n_samples, n_encoded_features)\n",
      " |          Transformed input. If `sparse=True`, a sparse matrix will be\n",
      " |          returned.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "help(OneHotEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categories: [array(['Female', 'Male', 'Unknown'], dtype=object), array(['Chrome', 'Internet Explorer', 'Safari'], dtype=object)]\n",
      "feature names: ['gender_Female' 'gender_Male' 'gender_Unknown' 'browser_Chrome'\n",
      " 'browser_Internet Explorer' 'browser_Safari']\n",
      "X_train transformed\n",
      "[[0. 1. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 1. 0.]\n",
      " [0. 1. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 0.]]\n",
      "X_test transformed\n",
      "[[1. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# toy example\n",
    "train = {'gender':['Male','Female','Unknown','Male','Female','Female'],\\\n",
    "         'browser':['Safari','Safari','Internet Explorer','Chrome','Chrome','Internet Explorer']}\n",
    "test = {'gender':['Female','Male','Unknown','Female'],'browser':['Chrome','Firefox','Internet Explorer','Safari']}\n",
    "\n",
    "Xtoy_train = pd.DataFrame(train)\n",
    "Xtoy_test = pd.DataFrame(test)\n",
    "\n",
    "ftrs = ['gender','browser']\n",
    "\n",
    "# initialize the encoder\n",
    "enc = OneHotEncoder(sparse=False,handle_unknown='ignore') # by default, OneHotEncoder returns a sparse matrix. sparse=False returns a 2D array\n",
    "# fit the training data\n",
    "enc.fit(Xtoy_train)\n",
    "print('categories:',enc.categories_)\n",
    "print('feature names:',enc.get_feature_names(ftrs))\n",
    "# transform X_train\n",
    "X_train_ohe = enc.transform(Xtoy_train)\n",
    "#print(X_train_ohe)\n",
    "# do all of this in one step\n",
    "X_train_ohe = enc.fit_transform(Xtoy_train)\n",
    "print('X_train transformed')\n",
    "print(X_train_ohe)\n",
    "\n",
    "# transform X_test\n",
    "X_test_ohe = enc.transform(Xtoy_test)\n",
    "print('X_test transformed')\n",
    "print(X_test_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature names: ['workclass_ ?' 'workclass_ Federal-gov' 'workclass_ Local-gov'\n",
      " 'workclass_ Never-worked' 'workclass_ Private' 'workclass_ Self-emp-inc'\n",
      " 'workclass_ Self-emp-not-inc' 'workclass_ State-gov'\n",
      " 'workclass_ Without-pay' 'marital-status_ Divorced'\n",
      " 'marital-status_ Married-AF-spouse' 'marital-status_ Married-civ-spouse'\n",
      " 'marital-status_ Married-spouse-absent' 'marital-status_ Never-married'\n",
      " 'marital-status_ Separated' 'marital-status_ Widowed' 'occupation_ ?'\n",
      " 'occupation_ Adm-clerical' 'occupation_ Armed-Forces'\n",
      " 'occupation_ Craft-repair' 'occupation_ Exec-managerial'\n",
      " 'occupation_ Farming-fishing' 'occupation_ Handlers-cleaners'\n",
      " 'occupation_ Machine-op-inspct' 'occupation_ Other-service'\n",
      " 'occupation_ Priv-house-serv' 'occupation_ Prof-specialty'\n",
      " 'occupation_ Protective-serv' 'occupation_ Sales'\n",
      " 'occupation_ Tech-support' 'occupation_ Transport-moving'\n",
      " 'relationship_ Husband' 'relationship_ Not-in-family'\n",
      " 'relationship_ Other-relative' 'relationship_ Own-child'\n",
      " 'relationship_ Unmarried' 'relationship_ Wife' 'race_ Amer-Indian-Eskimo'\n",
      " 'race_ Asian-Pac-Islander' 'race_ Black' 'race_ Other' 'race_ White'\n",
      " 'sex_ Female' 'sex_ Male' 'native-country_ ?' 'native-country_ Cambodia'\n",
      " 'native-country_ Canada' 'native-country_ China'\n",
      " 'native-country_ Columbia' 'native-country_ Cuba'\n",
      " 'native-country_ Dominican-Republic' 'native-country_ Ecuador'\n",
      " 'native-country_ El-Salvador' 'native-country_ England'\n",
      " 'native-country_ France' 'native-country_ Germany'\n",
      " 'native-country_ Greece' 'native-country_ Guatemala'\n",
      " 'native-country_ Haiti' 'native-country_ Holand-Netherlands'\n",
      " 'native-country_ Honduras' 'native-country_ Hong'\n",
      " 'native-country_ Hungary' 'native-country_ India' 'native-country_ Iran'\n",
      " 'native-country_ Ireland' 'native-country_ Italy'\n",
      " 'native-country_ Jamaica' 'native-country_ Japan' 'native-country_ Laos'\n",
      " 'native-country_ Mexico' 'native-country_ Nicaragua'\n",
      " 'native-country_ Outlying-US(Guam-USVI-etc)' 'native-country_ Peru'\n",
      " 'native-country_ Philippines' 'native-country_ Poland'\n",
      " 'native-country_ Portugal' 'native-country_ Puerto-Rico'\n",
      " 'native-country_ Scotland' 'native-country_ South'\n",
      " 'native-country_ Taiwan' 'native-country_ Thailand'\n",
      " 'native-country_ Trinadad&Tobago' 'native-country_ United-States'\n",
      " 'native-country_ Vietnam' 'native-country_ Yugoslavia']\n",
      "86\n"
     ]
    }
   ],
   "source": [
    "# apply OHE to the adult dataset\n",
    "\n",
    "# let's collect all categorical features first\n",
    "onehot_ftrs = ['workclass','marital-status','occupation','relationship','race','sex','native-country']\n",
    "# initialize the encoder\n",
    "enc = OneHotEncoder(sparse=False,handle_unknown='ignore') # by default, OneHotEncoder returns a sparse matrix. sparse=False returns a 2D array\n",
    "# fit the training data\n",
    "enc.fit(X_train[onehot_ftrs])\n",
    "print('feature names:',enc.get_feature_names(onehot_ftrs))\n",
    "print(len(enc.get_feature_names(onehot_ftrs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformed train features:\n",
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]]\n",
      "transformed val features:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]]\n",
      "transformed test features:\n",
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# transform X_train\n",
    "onehot_train = enc.transform(X_train[onehot_ftrs])\n",
    "print('transformed train features:')\n",
    "print(onehot_train)\n",
    "# transform X_val\n",
    "onehot_val = enc.transform(X_val[onehot_ftrs])\n",
    "print('transformed val features:')\n",
    "print(onehot_val)\n",
    "# transform X_test\n",
    "onehot_test = enc.transform(X_test[onehot_ftrs])\n",
    "print('transformed test features:')\n",
    "print(onehot_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quiz 1\n",
    "Which of the following categorical features should be preprocessed with the Ordinal Encoder?\n",
    "- marital status (Married, Divorced, Never-married, Separated, Widowed)\n",
    "- exterior quality of a house (Excellent, Good, Average/Typical, Fair, Poor)\n",
    "- native country (USA, Hungary, China, India, Germany)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='LIGHTGRAY'>By the end of this lecture, you will be able to</font>\n",
    "- <font color='LIGHTGRAY'>apply one-hot encoding or ordinal encoding to categorical variables</font>\n",
    "- **apply scaling and normalization to continuous variables**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Continuous features: MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If the continuous feature values are reasonably bounded, MinMaxScaler is a good way to scale the features.\n",
    "- Age is expected to be within the range of 0 and 100.\n",
    "- Number of hours worked per week is in the range of 0 to 80.\n",
    "- If unsure, plot the histogram of the feature to verify or just go with the standard scaler!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MinMaxScaler in module sklearn.preprocessing._data:\n",
      "\n",
      "class MinMaxScaler(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  MinMaxScaler(feature_range=(0, 1), *, copy=True, clip=False)\n",
      " |  \n",
      " |  Transform features by scaling each feature to a given range.\n",
      " |  \n",
      " |  This estimator scales and translates each feature individually such\n",
      " |  that it is in the given range on the training set, e.g. between\n",
      " |  zero and one.\n",
      " |  \n",
      " |  The transformation is given by::\n",
      " |  \n",
      " |      X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
      " |      X_scaled = X_std * (max - min) + min\n",
      " |  \n",
      " |  where min, max = feature_range.\n",
      " |  \n",
      " |  This transformation is often used as an alternative to zero mean,\n",
      " |  unit variance scaling.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  feature_range : tuple (min, max), default=(0, 1)\n",
      " |      Desired range of transformed data.\n",
      " |  \n",
      " |  copy : bool, default=True\n",
      " |      Set to False to perform inplace row normalization and avoid a\n",
      " |      copy (if the input is already a numpy array).\n",
      " |  \n",
      " |  clip : bool, default=False\n",
      " |      Set to True to clip transformed values of held-out data to\n",
      " |      provided `feature range`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  min_ : ndarray of shape (n_features,)\n",
      " |      Per feature adjustment for minimum. Equivalent to\n",
      " |      ``min - X.min(axis=0) * self.scale_``\n",
      " |  \n",
      " |  scale_ : ndarray of shape (n_features,)\n",
      " |      Per feature relative scaling of the data. Equivalent to\n",
      " |      ``(max - min) / (X.max(axis=0) - X.min(axis=0))``\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *scale_* attribute.\n",
      " |  \n",
      " |  data_min_ : ndarray of shape (n_features,)\n",
      " |      Per feature minimum seen in the data\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *data_min_*\n",
      " |  \n",
      " |  data_max_ : ndarray of shape (n_features,)\n",
      " |      Per feature maximum seen in the data\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *data_max_*\n",
      " |  \n",
      " |  data_range_ : ndarray of shape (n_features,)\n",
      " |      Per feature range ``(data_max_ - data_min_)`` seen in the data\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *data_range_*\n",
      " |  \n",
      " |  n_samples_seen_ : int\n",
      " |      The number of samples processed by the estimator.\n",
      " |      It will be reset on new calls to fit, but increments across\n",
      " |      ``partial_fit`` calls.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.preprocessing import MinMaxScaler\n",
      " |  >>> data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
      " |  >>> scaler = MinMaxScaler()\n",
      " |  >>> print(scaler.fit(data))\n",
      " |  MinMaxScaler()\n",
      " |  >>> print(scaler.data_max_)\n",
      " |  [ 1. 18.]\n",
      " |  >>> print(scaler.transform(data))\n",
      " |  [[0.   0.  ]\n",
      " |   [0.25 0.25]\n",
      " |   [0.5  0.5 ]\n",
      " |   [1.   1.  ]]\n",
      " |  >>> print(scaler.transform([[2, 2]]))\n",
      " |  [[1.5 0. ]]\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  minmax_scale : Equivalent function without the estimator API.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      " |  transform.\n",
      " |  \n",
      " |  For a comparison of the different scalers, transformers, and normalizers,\n",
      " |  see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      " |  <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MinMaxScaler\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, feature_range=(0, 1), *, copy=True, clip=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Compute the minimum and maximum to be used for later scaling.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The data used to compute the per-feature minimum and maximum\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted scaler.\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Undo the scaling of X according to feature_range.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input data that will be transformed. It cannot be sparse.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Xt : ndarray of shape (n_samples, n_features)\n",
      " |          Transformed data.\n",
      " |  \n",
      " |  partial_fit(self, X, y=None)\n",
      " |      Online computation of min and max on X for later scaling.\n",
      " |      \n",
      " |      All of X is processed as a single batch. This is intended for cases\n",
      " |      when :meth:`fit` is not feasible due to very large number of\n",
      " |      `n_samples` or because X is read from a continuous stream.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The data used to compute the mean and standard deviation\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted scaler.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Scale features of X according to feature_range.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input data that will be transformed.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Xt : ndarray of shape (n_samples, n_features)\n",
      " |          Transformed data.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      " |      and returns a transformed version of `X`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input samples.\n",
      " |      \n",
      " |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      " |          Target values (None for unsupervised transformations).\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "help(MinMaxScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.30645161 0.        ]\n",
      " [0.83870968 0.66666667]\n",
      " [0.         0.16666667]\n",
      " [0.88709677 1.        ]\n",
      " [0.46774194 0.66666667]\n",
      " [1.         0.33333333]\n",
      " [0.30645161 0.66666667]]\n",
      "[[ 1.12903226  0.        ]\n",
      " [ 0.20967742  0.66666667]\n",
      " [-0.0483871   0.        ]\n",
      " [ 0.75806452  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# toy data\n",
    "# let's assume we have two continuous features:\n",
    "train = {'age':[32,65,13,68,42,75,32],'number of hours worked':[0,40,10,60,40,20,40]}\n",
    "test = {'age':[83,26,10,60],'number of hours worked':[0,40,0,60]}\n",
    "\n",
    "# (value - min) / (max - min), if value is 32, min is 13 and max is 75, then we have 19 / 62 = 0.3064\n",
    "\n",
    "Xtoy_train = pd.DataFrame(train)\n",
    "Xtoy_test = pd.DataFrame(test)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(Xtoy_train)\n",
    "print(scaler.transform(Xtoy_train))\n",
    "print(scaler.transform(Xtoy_test)) # note how scaled X_test contains values larger than 1 and smaller than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.19178082 0.39795918]\n",
      " [0.32876712 0.39795918]\n",
      " [0.60273973 0.5       ]\n",
      " ...\n",
      " [0.01369863 0.19387755]\n",
      " [0.45205479 0.84693878]\n",
      " [0.23287671 0.60204082]]\n",
      "[[0.35616438 0.5       ]\n",
      " [0.68493151 0.39795918]\n",
      " [0.09589041 0.39795918]\n",
      " ...\n",
      " [0.09589041 0.19387755]\n",
      " [0.02739726 0.44897959]\n",
      " [0.38356164 0.39795918]]\n",
      "[[0.06849315 0.39795918]\n",
      " [0.23287671 0.39795918]\n",
      " [0.43835616 0.5       ]\n",
      " ...\n",
      " [0.20547945 0.39795918]\n",
      " [0.21917808 0.37755102]\n",
      " [0.08219178 0.35714286]]\n"
     ]
    }
   ],
   "source": [
    "# adult data\n",
    "\n",
    "minmax_ftrs = ['age','hours-per-week']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train[minmax_ftrs])\n",
    "print(scaler.transform(X_train[minmax_ftrs]))\n",
    "print(scaler.transform(X_val[minmax_ftrs])) \n",
    "print(scaler.transform(X_test[minmax_ftrs])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Continuous features: StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If the continuous feature values follow a tailed distribution, StandardScaler is better to use!\n",
    "- Salaries are a good example. Most people earn less than 100k but there are a small number of super-rich people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class StandardScaler in module sklearn.preprocessing._data:\n",
      "\n",
      "class StandardScaler(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  StandardScaler(*, copy=True, with_mean=True, with_std=True)\n",
      " |  \n",
      " |  Standardize features by removing the mean and scaling to unit variance\n",
      " |  \n",
      " |  The standard score of a sample `x` is calculated as:\n",
      " |  \n",
      " |      z = (x - u) / s\n",
      " |  \n",
      " |  where `u` is the mean of the training samples or zero if `with_mean=False`,\n",
      " |  and `s` is the standard deviation of the training samples or one if\n",
      " |  `with_std=False`.\n",
      " |  \n",
      " |  Centering and scaling happen independently on each feature by computing\n",
      " |  the relevant statistics on the samples in the training set. Mean and\n",
      " |  standard deviation are then stored to be used on later data using\n",
      " |  :meth:`transform`.\n",
      " |  \n",
      " |  Standardization of a dataset is a common requirement for many\n",
      " |  machine learning estimators: they might behave badly if the\n",
      " |  individual features do not more or less look like standard normally\n",
      " |  distributed data (e.g. Gaussian with 0 mean and unit variance).\n",
      " |  \n",
      " |  For instance many elements used in the objective function of\n",
      " |  a learning algorithm (such as the RBF kernel of Support Vector\n",
      " |  Machines or the L1 and L2 regularizers of linear models) assume that\n",
      " |  all features are centered around 0 and have variance in the same\n",
      " |  order. If a feature has a variance that is orders of magnitude larger\n",
      " |  that others, it might dominate the objective function and make the\n",
      " |  estimator unable to learn from other features correctly as expected.\n",
      " |  \n",
      " |  This scaler can also be applied to sparse CSR or CSC matrices by passing\n",
      " |  `with_mean=False` to avoid breaking the sparsity structure of the data.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  copy : bool, default=True\n",
      " |      If False, try to avoid a copy and do inplace scaling instead.\n",
      " |      This is not guaranteed to always work inplace; e.g. if the data is\n",
      " |      not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n",
      " |      returned.\n",
      " |  \n",
      " |  with_mean : bool, default=True\n",
      " |      If True, center the data before scaling.\n",
      " |      This does not work (and will raise an exception) when attempted on\n",
      " |      sparse matrices, because centering them entails building a dense\n",
      " |      matrix which in common use cases is likely to be too large to fit in\n",
      " |      memory.\n",
      " |  \n",
      " |  with_std : bool, default=True\n",
      " |      If True, scale the data to unit variance (or equivalently,\n",
      " |      unit standard deviation).\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  scale_ : ndarray of shape (n_features,) or None\n",
      " |      Per feature relative scaling of the data to achieve zero mean and unit\n",
      " |      variance. Generally this is calculated using `np.sqrt(var_)`. If a\n",
      " |      variance is zero, we can't achieve unit variance, and the data is left\n",
      " |      as-is, giving a scaling factor of 1. `scale_` is equal to `None`\n",
      " |      when `with_std=False`.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *scale_*\n",
      " |  \n",
      " |  mean_ : ndarray of shape (n_features,) or None\n",
      " |      The mean value for each feature in the training set.\n",
      " |      Equal to ``None`` when ``with_mean=False``.\n",
      " |  \n",
      " |  var_ : ndarray of shape (n_features,) or None\n",
      " |      The variance for each feature in the training set. Used to compute\n",
      " |      `scale_`. Equal to ``None`` when ``with_std=False``.\n",
      " |  \n",
      " |  n_samples_seen_ : int or ndarray of shape (n_features,)\n",
      " |      The number of samples processed by the estimator for each feature.\n",
      " |      If there are no missing samples, the ``n_samples_seen`` will be an\n",
      " |      integer, otherwise it will be an array of dtype int. If\n",
      " |      `sample_weights` are used it will be a float (if no missing data)\n",
      " |      or an array of dtype float that sums the weights seen so far.\n",
      " |      Will be reset on new calls to fit, but increments across\n",
      " |      ``partial_fit`` calls.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.preprocessing import StandardScaler\n",
      " |  >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n",
      " |  >>> scaler = StandardScaler()\n",
      " |  >>> print(scaler.fit(data))\n",
      " |  StandardScaler()\n",
      " |  >>> print(scaler.mean_)\n",
      " |  [0.5 0.5]\n",
      " |  >>> print(scaler.transform(data))\n",
      " |  [[-1. -1.]\n",
      " |   [-1. -1.]\n",
      " |   [ 1.  1.]\n",
      " |   [ 1.  1.]]\n",
      " |  >>> print(scaler.transform([[2, 2]]))\n",
      " |  [[3. 3.]]\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  scale : Equivalent function without the estimator API.\n",
      " |  \n",
      " |  :class:`~sklearn.decomposition.PCA` : Further removes the linear\n",
      " |      correlation across features with 'whiten=True'.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      " |  transform.\n",
      " |  \n",
      " |  We use a biased estimator for the standard deviation, equivalent to\n",
      " |  `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\n",
      " |  affect model performance.\n",
      " |  \n",
      " |  For a comparison of the different scalers, transformers, and normalizers,\n",
      " |  see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      " |  <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      StandardScaler\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, copy=True, with_mean=True, with_std=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None, sample_weight=None)\n",
      " |      Compute the mean and std to be used for later scaling.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The data used to compute the mean and standard deviation\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Individual weights for each sample.\n",
      " |      \n",
      " |          .. versionadded:: 0.24\n",
      " |             parameter *sample_weight* support to StandardScaler.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted scaler.\n",
      " |  \n",
      " |  inverse_transform(self, X, copy=None)\n",
      " |      Scale back the data to the original representation\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The data used to scale along the features axis.\n",
      " |      copy : bool, default=None\n",
      " |          Copy the input X or not.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  partial_fit(self, X, y=None, sample_weight=None)\n",
      " |      Online computation of mean and std on X for later scaling.\n",
      " |      \n",
      " |      All of X is processed as a single batch. This is intended for cases\n",
      " |      when :meth:`fit` is not feasible due to very large number of\n",
      " |      `n_samples` or because X is read from a continuous stream.\n",
      " |      \n",
      " |      The algorithm for incremental mean and std is given in Equation 1.5a,b\n",
      " |      in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. \"Algorithms\n",
      " |      for computing the sample variance: Analysis and recommendations.\"\n",
      " |      The American Statistician 37.3 (1983): 242-247:\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The data used to compute the mean and standard deviation\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Individual weights for each sample.\n",
      " |      \n",
      " |          .. versionadded:: 0.24\n",
      " |             parameter *sample_weight* support to StandardScaler.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted scaler.\n",
      " |  \n",
      " |  transform(self, X, copy=None)\n",
      " |      Perform standardization by centering and scaling\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix of shape (n_samples, n_features)\n",
      " |          The data used to scale along the features axis.\n",
      " |      copy : bool, default=None\n",
      " |          Copy the input X or not.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      " |      and returns a transformed version of `X`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input samples.\n",
      " |      \n",
      " |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      " |          Target values (None for unsupervised transformations).\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "help(StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.44873188]\n",
      " [-0.36895732]\n",
      " [-0.4806417 ]\n",
      " [ 2.58270127]\n",
      " [-0.51255153]\n",
      " [ 0.18946457]\n",
      " [-0.49659661]\n",
      " [-0.46468679]]\n",
      "[[-0.52850644]\n",
      " [-0.43277697]\n",
      " [ 4.1781924 ]\n",
      " [-0.41682206]]\n"
     ]
    }
   ],
   "source": [
    "# toy data\n",
    "train = {'salary':[50_000,75_000,40_000,1_000_000,30_000,250_000,35_000,45_000]}\n",
    "test = {'salary':[25_000,55_000,1_500_000,60_000]}\n",
    "\n",
    "Xtoy_train = pd.DataFrame(train)\n",
    "Xtoy_test = pd.DataFrame(test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "print(scaler.fit_transform(Xtoy_train))\n",
    "print(scaler.transform(Xtoy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.14633293 -0.22318878]\n",
      " [-0.14633293 -0.22318878]\n",
      " [-0.14633293 -0.22318878]\n",
      " ...\n",
      " [-0.14633293 -0.22318878]\n",
      " [-0.14633293 -0.22318878]\n",
      " [-0.14633293 -0.22318878]]\n",
      "[[-0.14633293 -0.22318878]\n",
      " [-0.14633293 -0.22318878]\n",
      " [-0.14633293 -0.22318878]\n",
      " ...\n",
      " [-0.14633293 -0.22318878]\n",
      " [-0.14633293 -0.22318878]\n",
      " [-0.14633293 -0.22318878]]\n",
      "[[-0.14633293 -0.22318878]\n",
      " [-0.14633293 -0.22318878]\n",
      " [-0.14633293 -0.22318878]\n",
      " ...\n",
      " [-0.14633293 -0.22318878]\n",
      " [-0.14633293 -0.22318878]\n",
      " [-0.14633293 -0.22318878]]\n"
     ]
    }
   ],
   "source": [
    "# adult data\n",
    "\n",
    "std_ftrs = ['capital-gain','capital-loss']\n",
    "scaler = StandardScaler()\n",
    "print(scaler.fit_transform(X_train[std_ftrs]))\n",
    "print(scaler.transform(X_val[std_ftrs]))\n",
    "print(scaler.transform(X_test[std_ftrs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lecture 7, Quiz 1 on Canvas\n",
    "\n",
    "Which of these features could be safely preprocessed by the minmax scaler?\n",
    "- number of minutes spent on the website in a day\n",
    "- number of days a year spent abroad in a year\n",
    "- USD donated to charity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How and when to do preprocessing in the ML pipeline?\n",
    "\n",
    "- **APPLY TRANSFORMER.FIT ONLY ON YOUR TRAINING DATA!** Then transform the validation and test sets.\n",
    "- One of the most common mistake practitioners make is leaking statistics!\n",
    "     - fit_transform is applied to the whole dataset, then the data is split into train/validation/test\n",
    "         - this is wrong because the test set statistics impacts how the training and validation sets are transformed\n",
    "         - but the test set must be separated by train and val, and val must be separated by train\n",
    "     - or fit_transform is applied to the train, then fit_transform is applied to the validation set, and fit_transform is applied to the test set\n",
    "         - this is wrong because the relative position of the points change\n",
    "<center><img src=\"figures/no_separate_scaling.png\" width=\"1200\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scikit-learn's pipelines\n",
    "\n",
    "- The steps in the ML pipleine can be chained together into a scikit-learn pipeline which consists of transformers and one final estimator which is usually your classifier or regression model.\n",
    "- It neatly combines the preprocessing steps and it helps to avoid leaking statistics.\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "df = pd.read_csv('data/adult_data.csv')\n",
    "\n",
    "# let's separate the feature matrix X, and target variable y\n",
    "y = df['gross-income'] # remember, we want to predict who earns more than 50k or less than 50k\n",
    "X = df.loc[:, df.columns != 'gross-income'] # all other columns are features\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "# first split to separate out the training set\n",
    "X_train, X_other, y_train, y_other = train_test_split(X,y,train_size = 0.6,random_state=random_state)\n",
    "\n",
    "# second split to separate out the validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_other,y_other,train_size = 0.5,random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19536, 14)\n",
      "(19536, 91)\n",
      "[[10.          0.          0.         ...  0.39795918 -0.14633293\n",
      "  -0.22318878]\n",
      " [ 9.          0.          0.         ...  0.39795918 -0.14633293\n",
      "  -0.22318878]\n",
      " [ 8.          0.          0.         ...  0.5        -0.14633293\n",
      "  -0.22318878]\n",
      " ...\n",
      " [ 6.          0.          0.         ...  0.19387755 -0.14633293\n",
      "  -0.22318878]\n",
      " [ 8.          0.          0.         ...  0.84693878 -0.14633293\n",
      "  -0.22318878]\n",
      " [12.          0.          0.         ...  0.60204082 -0.14633293\n",
      "  -0.22318878]]\n"
     ]
    }
   ],
   "source": [
    "# collect which encoder to use on each feature\n",
    "# needs to be done manually\n",
    "ordinal_ftrs = ['education'] \n",
    "ordinal_cats = [[' Preschool',' 1st-4th',' 5th-6th',' 7th-8th',' 9th',' 10th',' 11th',' 12th',' HS-grad',\\\n",
    "                ' Some-college',' Assoc-voc',' Assoc-acdm',' Bachelors',' Masters',' Prof-school',' Doctorate']]\n",
    "onehot_ftrs = ['workclass','marital-status','occupation','relationship','race','sex','native-country']\n",
    "minmax_ftrs = ['age','hours-per-week']\n",
    "std_ftrs = ['capital-gain','capital-loss']\n",
    "\n",
    "# collect all the encoders\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('ord', OrdinalEncoder(categories = ordinal_cats), ordinal_ftrs),\n",
    "        ('onehot', OneHotEncoder(sparse=False,handle_unknown='ignore'), onehot_ftrs),\n",
    "        ('minmax', MinMaxScaler(), minmax_ftrs),\n",
    "        ('std', StandardScaler(), std_ftrs)])\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor)]) # for now we only preprocess \n",
    "                                                       # later on we will add other steps here\n",
    "\n",
    "X_train_prep = clf.fit_transform(X_train)\n",
    "X_val_prep = clf.transform(X_val)\n",
    "X_test_prep = clf.transform(X_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_train_prep.shape)\n",
    "print(X_train_prep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
