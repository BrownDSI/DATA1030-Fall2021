{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mud card answers\n",
    "- **how do we know if we should gather more data for SVM? I believe you mentioned something about this, but I didn't understand†!**\n",
    "    - it's not just for SVMs but generally for any ML model\n",
    "    - you create a [learning curve](https://scikit-learn.org/stable/modules/learning_curve.html#learning-curve)\n",
    "    - if the curve is steep, it makes sense to collect more data if you can\n",
    "    - if the learning curve is saturated, more data will likely not improve model performance\n",
    "- **What does RBF kernel look like in high dimension**\n",
    "- **bit confused about how gaussians are 2d in svm classification!**\n",
    "    - it's still a gaussian but in higher dimensions :)\n",
    "    - it's called the [multivariate normal distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution)\n",
    "- **What is the danger of overfitting with trees? Even if the decision boundary is some complicated fractal-esc line it seems like you would have similar accuracy to a less complex decision boundary.**\n",
    "    - not really, a model that overfits performs very well on the training set but poorly on the validation set\n",
    "    - that's still true for trees\n",
    "    - the complex fractal-esc line occurs because the model is trying to fit each individual point so it wiggles around each training sample\n",
    "    - such a model will not do well on validation samples that are at different locations\n",
    "- **I still don't understand the max_depth in the RandomForest algorithms.¬† If the max_depth is 3, does it mean we have 3 turning points in the graph you plotted in the lecture notes?**\n",
    "    - nope, max 8 but it can be less\n",
    "    - max depth of 1 means 2 turning points\n",
    "    - max depth of 2 means 4 turning points\n",
    "    - max depth of n means 2^n turning points\n",
    "    - as we discussed in class, trees cannot be arbitrarily deep so the 2^n is just an upper limit, the actual number of turning points can be less but it cannot be more\n",
    "- **For max_depth,...these parameters, we just try some values and pick the best one?**\n",
    "    - well yes\n",
    "    - but you need to be mindful what values you try\n",
    "- **\"With sklearn random forests,¬† we tune n_estimators after other best hyperparameters are found.**\n",
    "    - basically yes\n",
    "- **is it possible that the best hyperparameters change when the n_estimator increase? then the model would not be the best one\"\n",
    "    - in my experience, that doesn't happen\n",
    "    - but write code and verify it on you project dataset\n",
    "- **Still don't quite understand how the Gaussian kernel comes into play for the SVR and SVC: I know the points are \"smeared\" with the density of the kernel, but I don't see how that drastically changes the behavior of the SVM.**\n",
    "    - check out [this page](https://scikit-learn.org/stable/modules/svm.html#svm-classification) and the examples\n",
    "- **How many hyperparameter values do you recommend looping through when working with larger models?**\n",
    "    - I assume you mean larger datasets\n",
    "    - I know you expect a number like 20 but there is no such number\n",
    "    - as many as your computing resources allow\n",
    "    - if you need more guidance, come to my office hours\n",
    "- **what happens if we have a high gamma in SVC? In the class note, you plotted 4 graphs of different gamma in SVC, for gamma =1e4, there is no \"sorted line\" in the graph like other 3, where is the \"sorted line\"?**\n",
    "    - I don't know what you mean by 'sorted line'\n",
    "    - can you come to the office hours or post on ed discussion?\n",
    "- **while tuning models in real data science projects, is it correct that what we need to do is just make a list of alpha and plot all accuracy scores and pick the best one?**\n",
    "    - you might have more than one hyperparameters to tune\n",
    "    - you might not use accuracy but a different evaluation metric\n",
    "    - you need to choose the best model based on the validation scores not just any scores\n",
    "- **When creating a learning curve, would we change the size of our training set multiple times and calculate the eval metric for each to fill in the points on the x-axis for sample sizes up to the size of our dataset?**\n",
    "    - yes\n",
    "    - and ideally you would calculate the eval metric multiple times for each size value and you'd plot the mean and std of the scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The supervised ML pipeline\n",
    "The goal: Use the training data (X and y) to develop a <font color='red'>model</font> which can <font color='red'>accurately</font> predict the target variable (y_new') for previously unseen data (X_new).\n",
    "\n",
    "**1. Exploratory Data Analysis (EDA)**: you need to understand your data and verify that it doesn't contain errors\n",
    "   - do as much EDA as you can!\n",
    "    \n",
    "**2. Split the data into different sets**: most often the sets are train, validation, and test (or holdout)\n",
    "   - practitioners often make errors in this step!\n",
    "   - you can split the data randomly, based on groups, based on time, or any other non-standard way if necessary to answer your ML question\n",
    "\n",
    "**3. Preprocess the data**: ML models only work if X and Y are numbers! Some ML models additionally require each feature to have 0 mean and 1 standard deviation (standardized features)\n",
    "   - often the original features you get contain strings (for example a gender feature would contain 'male', 'female', 'non-binary', 'unknown') which needs to transformed into numbers\n",
    "   - often the features are not standardized (e.g., age is between 0 and 100) but it needs to be standardized\n",
    "    \n",
    "**4. Choose an evaluation metric**: depends on the priorities of the stakeholders\n",
    "   - often requires quite a bit of thinking and ethical considerations\n",
    "     \n",
    "**5. Choose one or more ML techniques**: it is highly recommended that you try multiple models\n",
    "   - start with simple models like linear or logistic regression\n",
    "   - try also more complex models like nearest neighbors, support vector machines, random forest, etc.\n",
    "    \n",
    "<span style=\"background-color: #FFFF00\">**6. Tune the hyperparameters of your ML models (aka cross-validation)**</span>\n",
    "   - ML techniques have hyperparameters that you need to optimize to achieve best performance\n",
    "   - for each ML model, decide which parameters to tune and what values to try\n",
    "   - loop through each parameter combination\n",
    "       - train one model for each parameter combination\n",
    "       - evaluate how well the model performs on the validation set\n",
    "   - take the parameter combo that gives the best validation score\n",
    "   - evaluate that model on the test set to report how well the model is expected to perform on previously unseen data\n",
    "    \n",
    "**7. Interpret your model**: black boxes are often not useful\n",
    "   - check if your model uses features that make sense (excellent tool for debugging)\n",
    "   - often model predictions are not enough, you need to be able to explain how the model arrived to a particular prediction (e.g., in health care)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's put everything together\n",
    "- IID data first!\n",
    "- the adult dataset\n",
    "- the next two cells were copied from the week 3 material and slightly rewritten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, MinMaxScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('data/adult_data.csv')\n",
    "\n",
    "# let's separate the feature matrix X, and target variable y\n",
    "y = df['gross-income'] # remember, we want to predict who earns more than 50k or less than 50k\n",
    "X = df.loc[:, df.columns != 'gross-income'] # all other columns are features\n",
    "\n",
    "# collect which encoder to use on each feature\n",
    "# needs to be done manually\n",
    "ordinal_ftrs = ['education'] \n",
    "ordinal_cats = [[' Preschool',' 1st-4th',' 5th-6th',' 7th-8th',' 9th',' 10th',' 11th',' 12th',' HS-grad',\\\n",
    "                ' Some-college',' Assoc-voc',' Assoc-acdm',' Bachelors',' Masters',' Prof-school',' Doctorate']]\n",
    "onehot_ftrs = ['workclass','marital-status','occupation','relationship','race','sex','native-country']\n",
    "minmax_ftrs = ['age','hours-per-week']\n",
    "std_ftrs = ['capital-gain','capital-loss']\n",
    "\n",
    "# collect all the encoders into one preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('ord', OrdinalEncoder(categories = ordinal_cats), ordinal_ftrs),\n",
    "        ('onehot', OneHotEncoder(sparse=False,handle_unknown='ignore'), onehot_ftrs),\n",
    "        ('minmax', MinMaxScaler(), minmax_ftrs),\n",
    "        ('std', StandardScaler(), std_ftrs)])\n",
    "\n",
    "prep = Pipeline(steps=[('preprocessor', preprocessor)]) # for now we only preprocess, later we will add other steps here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "Let's recap preprocessing. Which of these statements are true?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "randoms state 1\n",
      "    {'max_features': 0.5, 'max_depth': 1}\n",
      "    0.7599815724815725 0.7581388206388207\n",
      "    {'max_features': 0.75, 'max_depth': 1}\n",
      "    0.7599815724815725 0.7581388206388207\n",
      "    {'max_features': 1.0, 'max_depth': 1}\n",
      "    0.7599815724815725 0.7581388206388207\n",
      "    {'max_features': 0.5, 'max_depth': 3}\n",
      "    0.8433149058149059 0.8465909090909091\n",
      "    {'max_features': 0.75, 'max_depth': 3}\n",
      "    0.842956592956593 0.8459766584766585\n",
      "    {'max_features': 1.0, 'max_depth': 3}\n",
      "    0.8421375921375921 0.8456695331695332\n",
      "    {'max_features': 0.5, 'max_depth': 10}\n",
      "    0.8763308763308764 0.8627149877149877\n",
      "    {'max_features': 0.75, 'max_depth': 10}\n",
      "    0.8761261261261262 0.8614864864864865\n",
      "    {'max_features': 1.0, 'max_depth': 10}\n",
      "    0.8761773136773137 0.8614864864864865\n",
      "    {'max_features': 0.5, 'max_depth': 30}\n",
      "    0.9797809172809173 0.8541154791154791\n",
      "    {'max_features': 0.75, 'max_depth': 30}\n",
      "    0.9807534807534808 0.850583538083538\n",
      "    {'max_features': 1.0, 'max_depth': 30}\n",
      "    0.9805487305487306 0.8495085995085995\n",
      "    {'max_features': 0.5, 'max_depth': 100}\n",
      "    0.9819819819819819 0.851044226044226\n",
      "    {'max_features': 0.75, 'max_depth': 100}\n",
      "    0.9819819819819819 0.8511977886977887\n",
      "    {'max_features': 1.0, 'max_depth': 100}\n",
      "    0.9819819819819819 0.8487407862407862\n",
      "best model parameters: {'max_features': 0.5, 'max_depth': 10}\n",
      "corresponding validation score: 0.8627149877149877\n",
      "test score: 0.8624289881774911\n",
      "randoms state 2\n",
      "    {'max_features': 0.5, 'max_depth': 1}\n",
      "    0.7904381654381655 0.788544226044226\n",
      "    {'max_features': 0.75, 'max_depth': 1}\n",
      "    0.7588554463554463 0.7547604422604423\n",
      "    {'max_features': 1.0, 'max_depth': 1}\n",
      "    0.7588554463554463 0.7547604422604423\n",
      "    {'max_features': 0.5, 'max_depth': 3}\n",
      "    0.8458742833742834 0.8398341523341524\n",
      "    {'max_features': 0.75, 'max_depth': 3}\n",
      "    0.8447481572481572 0.839527027027027\n",
      "    {'max_features': 1.0, 'max_depth': 3}\n",
      "    0.8448505323505323 0.8396805896805897\n",
      "    {'max_features': 0.5, 'max_depth': 10}\n",
      "    0.8781224406224406 0.8602579852579852\n",
      "    {'max_features': 0.75, 'max_depth': 10}\n",
      "    0.8779176904176904 0.8616400491400491\n",
      "    {'max_features': 1.0, 'max_depth': 10}\n",
      "    0.8778153153153153 0.859490171990172\n",
      "    {'max_features': 0.5, 'max_depth': 30}\n",
      "    0.9816748566748567 0.8508906633906634\n",
      "    {'max_features': 0.75, 'max_depth': 30}\n",
      "    0.9817772317772318 0.8498157248157249\n",
      "    {'max_features': 1.0, 'max_depth': 30}\n",
      "    0.9816236691236692 0.8487407862407862\n",
      "    {'max_features': 0.5, 'max_depth': 100}\n",
      "    0.9830569205569205 0.8468980343980343\n",
      "    {'max_features': 0.75, 'max_depth': 100}\n",
      "    0.9830569205569205 0.847512285012285\n",
      "    {'max_features': 1.0, 'max_depth': 100}\n",
      "    0.983005733005733 0.8459766584766585\n",
      "best model parameters: {'max_features': 0.75, 'max_depth': 10}\n",
      "corresponding validation score: 0.8616400491400491\n",
      "test score: 0.8615077537233226\n",
      "randoms state 3\n",
      "    {'max_features': 0.5, 'max_depth': 1}\n",
      "    0.7705773955773956 0.7627457002457002\n",
      "    {'max_features': 0.75, 'max_depth': 1}\n",
      "    0.7600839475839476 0.7530712530712531\n",
      "    {'max_features': 1.0, 'max_depth': 1}\n",
      "    0.7600839475839476 0.7530712530712531\n",
      "    {'max_features': 0.5, 'max_depth': 3}\n",
      "    0.846027846027846 0.8379914004914005\n",
      "    {'max_features': 0.75, 'max_depth': 3}\n",
      "    0.8456183456183456 0.8372235872235873\n",
      "    {'max_features': 1.0, 'max_depth': 3}\n",
      "    0.8456183456183456 0.8372235872235873\n",
      "    {'max_features': 0.5, 'max_depth': 10}\n",
      "    0.8778153153153153 0.8593366093366094\n",
      "    {'max_features': 0.75, 'max_depth': 10}\n",
      "    0.8767403767403767 0.859029484029484\n",
      "    {'max_features': 1.0, 'max_depth': 10}\n",
      "    0.8759213759213759 0.8588759213759214\n",
      "    {'max_features': 0.5, 'max_depth': 30}\n",
      "    0.9801392301392301 0.8541154791154791\n",
      "    {'max_features': 0.75, 'max_depth': 30}\n",
      "    0.9804463554463555 0.8539619164619164\n",
      "    {'max_features': 1.0, 'max_depth': 30}\n",
      "    0.9805999180999181 0.8507371007371007\n",
      "    {'max_features': 0.5, 'max_depth': 100}\n",
      "    0.9813677313677314 0.8516584766584766\n",
      "    {'max_features': 0.75, 'max_depth': 100}\n",
      "    0.9813165438165438 0.8507371007371007\n",
      "    {'max_features': 1.0, 'max_depth': 100}\n",
      "    0.9813677313677314 0.8482800982800983\n",
      "best model parameters: {'max_features': 0.5, 'max_depth': 10}\n",
      "corresponding validation score: 0.8593366093366094\n",
      "test score: 0.8635037617073545\n",
      "randoms state 4\n",
      "    {'max_features': 0.5, 'max_depth': 1}\n",
      "    0.7657145782145782 0.754914004914005\n",
      "    {'max_features': 0.75, 'max_depth': 1}\n",
      "    0.7657145782145782 0.754914004914005\n",
      "    {'max_features': 1.0, 'max_depth': 1}\n",
      "    0.7657145782145782 0.754914004914005\n",
      "    {'max_features': 0.5, 'max_depth': 3}\n",
      "    0.8479217854217854 0.8356879606879607\n",
      "    {'max_features': 0.75, 'max_depth': 3}\n",
      "    0.846488533988534 0.8361486486486487\n",
      "    {'max_features': 1.0, 'max_depth': 3}\n",
      "    0.846488533988534 0.836455773955774\n",
      "    {'max_features': 0.5, 'max_depth': 10}\n",
      "    0.8811936936936937 0.8584152334152334\n",
      "    {'max_features': 0.75, 'max_depth': 10}\n",
      "    0.8822686322686323 0.8591830466830467\n",
      "    {'max_features': 1.0, 'max_depth': 10}\n",
      "    0.883087633087633 0.8582616707616708\n",
      "    {'max_features': 0.5, 'max_depth': 30}\n",
      "    0.9817260442260443 0.8495085995085995\n",
      "    {'max_features': 0.75, 'max_depth': 30}\n",
      "    0.9822891072891073 0.8499692874692875\n",
      "    {'max_features': 1.0, 'max_depth': 30}\n",
      "    0.9823402948402948 0.847051597051597\n",
      "    {'max_features': 0.5, 'max_depth': 100}\n",
      "    0.9829545454545454 0.8476658476658476\n",
      "    {'max_features': 0.75, 'max_depth': 100}\n",
      "    0.9829545454545454 0.8481265356265356\n",
      "    {'max_features': 1.0, 'max_depth': 100}\n",
      "    0.9829545454545454 0.8462837837837838\n",
      "best model parameters: {'max_features': 0.75, 'max_depth': 10}\n",
      "corresponding validation score: 0.8591830466830467\n",
      "test score: 0.8601259020420697\n",
      "randoms state 5\n",
      "    {'max_features': 0.5, 'max_depth': 1}\n",
      "    0.7872133497133497 0.7926904176904177\n",
      "    {'max_features': 0.75, 'max_depth': 1}\n",
      "    0.756961506961507 0.7590601965601965\n",
      "    {'max_features': 1.0, 'max_depth': 1}\n",
      "    0.756961506961507 0.7590601965601965\n",
      "    {'max_features': 0.5, 'max_depth': 3}\n",
      "    0.842495904995905 0.8465909090909091\n",
      "    {'max_features': 0.75, 'max_depth': 3}\n",
      "    0.8420864045864046 0.8467444717444718\n",
      "    {'max_features': 1.0, 'max_depth': 3}\n",
      "    0.8420864045864046 0.8468980343980343\n",
      "    {'max_features': 0.5, 'max_depth': 10}\n",
      "    0.8764332514332515 0.8608722358722358\n",
      "    {'max_features': 0.75, 'max_depth': 10}\n",
      "    0.8766380016380017 0.860411547911548\n",
      "    {'max_features': 1.0, 'max_depth': 10}\n",
      "    0.8754095004095004 0.85995085995086\n",
      "    {'max_features': 0.5, 'max_depth': 30}\n",
      "    0.980497542997543 0.8527334152334153\n",
      "    {'max_features': 0.75, 'max_depth': 30}\n",
      "    0.9809070434070434 0.8495085995085995\n",
      "    {'max_features': 1.0, 'max_depth': 30}\n",
      "    0.9808046683046683 0.8482800982800983\n",
      "    {'max_features': 0.5, 'max_depth': 100}\n",
      "    0.9816748566748567 0.8502764127764127\n",
      "    {'max_features': 0.75, 'max_depth': 100}\n",
      "    0.9816748566748567 0.8490479115479116\n",
      "    {'max_features': 1.0, 'max_depth': 100}\n",
      "    0.9816236691236692 0.847972972972973\n",
      "best model parameters: {'max_features': 0.5, 'max_depth': 10}\n",
      "corresponding validation score: 0.8608722358722358\n",
      "test score: 0.8648856133886074\n"
     ]
    }
   ],
   "source": [
    "# let's train a random forest classifier\n",
    "\n",
    "# decide which parameters to tune and what values to try\n",
    "# all parameters not specified here will be the default\n",
    "param_grid = {\n",
    "              'max_depth': [1, 3, 10, 30, 100], # the max_depth should be smaller or equal than the number of features roughly\n",
    "              'max_features': [0.5,0.75,1.0] # linearly spaced between 0.5 and 1\n",
    "              } \n",
    "\n",
    "# we will loop through nr_states random states so we will return nr_states test scores and nr_states trained models\n",
    "nr_states = 5\n",
    "test_scores = np.zeros(nr_states)\n",
    "final_models = []\n",
    "\n",
    "# loop through the different random states\n",
    "for i in range(nr_states):\n",
    "    print('randoms state '+str(i+1))\n",
    "\n",
    "    # first split to separate out the training set\n",
    "    X_train, X_other, y_train, y_other = train_test_split(X,y,train_size = 0.6,random_state=42*i)\n",
    "\n",
    "    # second split to separate out the validation and test sets\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_other,y_other,train_size = 0.5,random_state=42*i)\n",
    "    \n",
    "    # preprocess the sets\n",
    "    X_train_prep = prep.fit_transform(X_train)\n",
    "    X_val_prep = prep.transform(X_val)\n",
    "    X_test_prep = prep.transform(X_test)\n",
    "\n",
    "    # we save the train and validation scores\n",
    "    # the validation scores are necessary to select the best model\n",
    "    # we save the train score just to check things\n",
    "    train_score = np.zeros(len(ParameterGrid(param_grid)))\n",
    "    val_score = np.zeros(len(ParameterGrid(param_grid)))\n",
    "    models = []\n",
    "    \n",
    "    # loop through all combinations of hyperparameter combos\n",
    "    for p in range(len(ParameterGrid(param_grid))):\n",
    "        params = ParameterGrid(param_grid)[p]\n",
    "        print('   ',params) \n",
    "        clf = RandomForestClassifier(**params,random_state = 42*i,n_jobs=-1) # initialize the classifier\n",
    "        clf.fit(X_train_prep,y_train) # fit the model\n",
    "        models.append(clf) # save it\n",
    "        # calculate train and validation accuracy scores\n",
    "        y_train_pred = clf.predict(X_train_prep)\n",
    "        train_score[p] = accuracy_score(y_train,y_train_pred)\n",
    "        y_val_pred = clf.predict(X_val_prep)\n",
    "        val_score[p] = accuracy_score(y_val,y_val_pred)\n",
    "        print('   ',train_score[p],val_score[p])\n",
    "    \n",
    "    # print out model parameters that maximize validation accuracy\n",
    "    print('best model parameters:',ParameterGrid(param_grid)[np.argmax(val_score)])\n",
    "    print('corresponding validation score:',np.max(val_score))\n",
    "    # collect and save the best model\n",
    "    final_models.append(models[np.argmax(val_score)])\n",
    "    # calculate and save the test score\n",
    "    y_test_pred = final_models[-1].predict(X_test_prep)\n",
    "    test_scores[i] = accuracy_score(y_test,y_test_pred)\n",
    "    print('test score:',test_scores[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to look out for\n",
    "- are the ranges of the hyperparameters wide enough?\n",
    "    - do you see underfitting? model performs poorly on both training and validation sets?\n",
    "    - do you see overfitting? model performs very good on training but worse on validation?\n",
    "    - if you don't see both, expand the range of the parameters and you'll likely find a better model\n",
    "    - read the manual and make sure you understand what the hyperparameter does in the model\n",
    "        - some parameters (like regularization parameters) should be linearly spaced in log [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2]\n",
    "        - some parameters (like max_features) should be linearly spaced\n",
    "- not every hyperparameter is equally important\n",
    "    - some parameters have little to no impact on train and validation scores\n",
    "    - in the example above, max_depth is much more important than max_features\n",
    "    - visualize the results if in doubt\n",
    "- is the best validation score similar to the test score?\n",
    "    - it's usual that the validation score is a bit larger than the test score\n",
    "    - but if the difference between the two scores is significant over multiple random states, something is off\n",
    "- traiv/val/test split is usually a safe bet for any splitting strategy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning with folds\n",
    "- the steps are a bit different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 15 candidates, totalling 60 fits\n",
      "best model parameters: {'randomforestclassifier__max_depth': 10, 'randomforestclassifier__max_features': 0.75}\n",
      "validation score: 0.8628685503685503\n",
      "test score: 0.8576692768309535\n",
      "Fitting 4 folds for each of 15 candidates, totalling 60 fits\n",
      "best model parameters: {'randomforestclassifier__max_depth': 10, 'randomforestclassifier__max_features': 0.75}\n",
      "validation score: 0.8601428132678133\n",
      "test score: 0.865806847842776\n",
      "Fitting 4 folds for each of 15 candidates, totalling 60 fits\n",
      "best model parameters: {'randomforestclassifier__max_depth': 10, 'randomforestclassifier__max_features': 0.5}\n",
      "validation score: 0.8624846437346437\n",
      "test score: 0.8590511285122063\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "df = pd.read_csv('data/adult_data.csv')\n",
    "\n",
    "# let's separate the feature matrix X, and target variable y\n",
    "y = df['gross-income'] # remember, we want to predict who earns more than 50k or less than 50k\n",
    "X = df.loc[:, df.columns != 'gross-income'] # all other columns are features\n",
    "\n",
    "ordinal_ftrs = ['education'] \n",
    "ordinal_cats = [[' Preschool',' 1st-4th',' 5th-6th',' 7th-8th',' 9th',' 10th',' 11th',' 12th',' HS-grad',\\\n",
    "                ' Some-college',' Assoc-voc',' Assoc-acdm',' Bachelors',' Masters',' Prof-school',' Doctorate']]\n",
    "onehot_ftrs = ['workclass','marital-status','occupation','relationship','race','sex','native-country']\n",
    "minmax_ftrs = ['age','hours-per-week']\n",
    "std_ftrs = ['capital-gain','capital-loss']\n",
    "\n",
    "# collect all the encoders\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('ord', OrdinalEncoder(categories = ordinal_cats), ordinal_ftrs),\n",
    "        ('onehot', OneHotEncoder(sparse=False,handle_unknown='ignore'), onehot_ftrs),\n",
    "        ('minmax', MinMaxScaler(), minmax_ftrs),\n",
    "        ('std', StandardScaler(), std_ftrs)])\n",
    "\n",
    "# all the same up to this point\n",
    "\n",
    "# we will use GridSearchCV and the parameter names need to contain the ML algorithm you want to use\n",
    "# the parameters of some ML algorithms have the same name and this is how we avoid confusion\n",
    "param_grid = {\n",
    "              'randomforestclassifier__max_depth': [1, 3, 10, 30, 100], # the max_depth should be smaller or equal than the number of features roughly\n",
    "              'randomforestclassifier__max_features': [0.5,0.75,1.0] # linearly spaced between 0.5 and 1\n",
    "              } \n",
    "\n",
    "nr_states = 3\n",
    "test_scores = np.zeros(nr_states)\n",
    "final_models = []\n",
    "\n",
    "for i in range(nr_states):\n",
    "    # first split to separate out the test set\n",
    "    # we will use kfold on other\n",
    "    X_other, X_test, y_other, y_test = train_test_split(X,y,test_size = 0.2,random_state=42*i)\n",
    "\n",
    "    # splitter for other\n",
    "    kf = KFold(n_splits=4,shuffle=True,random_state=42*i)\n",
    "\n",
    "    # the classifier\n",
    "    clf = RandomForestClassifier(random_state = 42*i) # initialize the classifier\n",
    "\n",
    "    # let's put together a pipeline\n",
    "    # the pipeline will fit_transform the training set (3 folds), and transform the last fold used as validation\n",
    "    # then it will train the ML algorithm on the training set and evaluate it on the validation set\n",
    "    # it repeats this step automatically such that each fold will be an evaluation set once\n",
    "    pipe = make_pipeline(preprocessor,clf)\n",
    "\n",
    "    # use GridSearchCV\n",
    "    # GridSearchCV loops through all parameter combinations and collects the results \n",
    "    grid = GridSearchCV(pipe, param_grid=param_grid,scoring = 'accuracy',\n",
    "                        cv=kf, return_train_score = True, n_jobs=-1, verbose=True)\n",
    "    \n",
    "    # this line actually fits the model on other\n",
    "    grid.fit(X_other, y_other)\n",
    "    # save results into a data frame. feel free to print it and inspect it\n",
    "    results = pd.DataFrame(grid.cv_results_)\n",
    "    #print(results)\n",
    "\n",
    "    print('best model parameters:',grid.best_params_)\n",
    "    print('validation score:',grid.best_score_) # this is the mean validation score over all iterations\n",
    "    # save the model\n",
    "    final_models.append(grid)\n",
    "    # calculate and save the test score\n",
    "    y_test_pred = final_models[-1].predict(X_test)\n",
    "    test_scores[i] = accuracy_score(y_test,y_test_pred)\n",
    "    print('test score:',test_scores[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to look out for\n",
    "- less code but more stuff is going on in the background hidden from you\n",
    "    - looping over multiple folds\n",
    "    - .fit_transform and .transform is hidden from you\n",
    "- nevertheless, GridSearchCV and pipelines are pretty powerful\n",
    "- working with folds is a bit more robust because the best hyperparameter is selected based on the average score of multiple trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "Can we use GridSearchCV with sets prepared by train_test_split in advance? Use the sklearn manual or stackoverflow to answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mud card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
