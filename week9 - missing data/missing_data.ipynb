{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mud card answers\n",
    "- **For the example of seizure dataset, I understand if the kfold CV is using blindly, the some datapoint of one patient will be distributed into different place, but I don't understand why that is the the bad case of data leakage, what happens if the new patient data will be seen for the first time**\n",
    "    - the model will perform poorly\n",
    "    - you can rewrite my code to simulate that scenario\n",
    "    - take data from two random patients and create a fourth set\n",
    "    - train a model as we did in class with the stratified split, and apply that model to the fourth set\n",
    "- **In the first example, seizure project data set. Is it a time series data? Because that we can see a time series pattern in the feature seizure_ID. If it is a time series data. Does it mean that we could apply group k-fold to deal with some time series data with groups**\n",
    "    - the measurements are time series\n",
    "    - but the features I calculated (mean, std, etc of 30 sec blocks) are not\n",
    "- **What's the best way to check if iid fails and should you do it for every dataset before you start the preprocessing step?**\n",
    "    - there is no statistical test for this\n",
    "    - but yes, it needs to be determined for every dataset\n",
    "    - you should do it before you start splitting because your splitting strategy depends on this\n",
    "    - generally, if you have some sort of ID in your dataset, it's likely not iid\n",
    "    - if you work with a time series data, it's not iid either\n",
    "- **I'm kind of confused how non time dependent features are built into an autoregression like we did in the lectures. Im finding a little difficult to get the lag matrix straight, and that seems like an added complexity**\n",
    "- **How do we reconcile a case where we have both time series and non time series features?**\n",
    "    - work out the indices of the lag matrix to get a better understanding or take a simple time series data and figure out what the lag matrix and the target variable are\n",
    "    - you'll have X1 (the non-time dependent feature matrix) and X2 (the time-dependent feature matrix) and you merge them into X\n",
    "- **are there applications where one might use the time series approach along a dimension other than time?**\n",
    "    - possibly?\n",
    "    - I have no clue what you have in mind :)\n",
    "- **Why the correlation with the date itself is one? I am sort of lost**\n",
    "    - write down that the equation of the Pearson correlation coefficient is (e.g., Eq 3 [here](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)) and work out what happens if x_i = y_i\n",
    "- **So after plotting the autocorrelation, we would find the lag i, that is not 0 but maximize the autocorrelation, and use it as the max shift? Or how do we use it?**\n",
    "    - you use it to check for any sort of periodicity or seasonality\n",
    "    - it also gives you an idea how many features you might need to use in the lag matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Missing data, part 2\n",
    "\n",
    "By the end of this module, you will be able to\n",
    "- review simple approaches for handling missing values\n",
    "- Apply XGBoost to a dataset with missing values\n",
    "- Apply the reduced-features model (also called the pattern submodel approach)\n",
    "- Decide which approach is best for your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## We continue working with the house price data set\n",
    "- regression problem\n",
    "- categorical, ordinal, continuous features\n",
    "- missing data in all feature types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 79)\n"
     ]
    }
   ],
   "source": [
    "# read the data\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Let's load the data\n",
    "df = pd.read_csv('data/train.csv')\n",
    "# drop the ID\n",
    "df.drop(columns=['Id'],inplace=True)\n",
    "\n",
    "# the target variable\n",
    "y = df['SalePrice']\n",
    "df.drop(columns=['SalePrice'],inplace=True)\n",
    "# the unprocessed feature matrix\n",
    "X = df.values\n",
    "print(X.shape)\n",
    "# the feature names\n",
    "ftrs = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction of missing values in features:\n",
      "LotFrontage     0.177397\n",
      "Alley           0.937671\n",
      "MasVnrType      0.005479\n",
      "MasVnrArea      0.005479\n",
      "BsmtQual        0.025342\n",
      "BsmtCond        0.025342\n",
      "BsmtExposure    0.026027\n",
      "BsmtFinType1    0.025342\n",
      "BsmtFinType2    0.026027\n",
      "Electrical      0.000685\n",
      "FireplaceQu     0.472603\n",
      "GarageType      0.055479\n",
      "GarageYrBlt     0.055479\n",
      "GarageFinish    0.055479\n",
      "GarageQual      0.055479\n",
      "GarageCond      0.055479\n",
      "PoolQC          0.995205\n",
      "Fence           0.807534\n",
      "MiscFeature     0.963014\n",
      "dtype: float64\n",
      "data types of the features with missing values:\n",
      "LotFrontage     float64\n",
      "Alley            object\n",
      "MasVnrType       object\n",
      "MasVnrArea      float64\n",
      "BsmtQual         object\n",
      "BsmtCond         object\n",
      "BsmtExposure     object\n",
      "BsmtFinType1     object\n",
      "BsmtFinType2     object\n",
      "Electrical       object\n",
      "FireplaceQu      object\n",
      "GarageType       object\n",
      "GarageYrBlt     float64\n",
      "GarageFinish     object\n",
      "GarageQual       object\n",
      "GarageCond       object\n",
      "PoolQC           object\n",
      "Fence            object\n",
      "MiscFeature      object\n",
      "dtype: object\n",
      "fraction of points with missing values: 1.0\n"
     ]
    }
   ],
   "source": [
    "perc_missing_per_ftr = df.isnull().sum(axis=0)/df.shape[0]\n",
    "print('fraction of missing values in features:')\n",
    "print(perc_missing_per_ftr[perc_missing_per_ftr > 0])\n",
    "print('data types of the features with missing values:')\n",
    "print(df[perc_missing_per_ftr[perc_missing_per_ftr > 0].index].dtypes)\n",
    "frac_missing = sum(df.isnull().sum(axis=1)!=0)/df.shape[0]\n",
    "print('fraction of points with missing values:',frac_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='LIGHTGRAY'>Missing data, part 2</font>\n",
    "\n",
    "<font color='LIGHTGRAY'>By the end of this lecture, you will be able to</font>\n",
    "- **review simple approaches for handling missing values**\n",
    "- <font color='LIGHTGRAY'>Apply XGBoost to a dataset with missing values</font>\n",
    "- <font color='LIGHTGRAY'>Apply the reduced-features model (also called the pattern submodel approach)</font>\n",
    "- <font color='LIGHTGRAY'>Decide which approach is best for your dataset</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple approaches for handling missing values\n",
    "- 1) categorical/ordinal features: treat missing values as another category\n",
    "    - missing values in categorical/ordinal features are not a big deal\n",
    "- 2) continuous features: this is the tough part\n",
    "    - sklearn's SimpleImputer\n",
    "- 3) exclude points or features with missing values\n",
    "    - might be OK\n",
    "- 4) multivariate imputation\n",
    "    - might be OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1a) Missing values in a categorical feature\n",
    "- YAY - this is not an issue at all!\n",
    "- Categorical feature needs to be one-hot encoded anyway\n",
    "- Just replace the missing values with 'NA' or 'missing' and treat it as a separate category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1b) Missing values in a ordinal feature\n",
    "- this can be a bit trickier but usually fine\n",
    "- Ordinal encoder is applied to ordinal features\n",
    "    - where does 'NA' or 'missing' fit into the order of the categories?\n",
    "    - usually first or last\n",
    "- if you can figure this out, you are fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(876, 79)\n",
      "(292, 79)\n",
      "(292, 79)\n"
     ]
    }
   ],
   "source": [
    "# let's split to train, CV, and test\n",
    "X_other, X_test, y_other, y_test = train_test_split(df, y, test_size=0.2, random_state=0)\n",
    "X_train, X_CV, y_train, y_CV = train_test_split(X_other, y_other, test_size=0.25, random_state=0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_CV.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# collect the various features\n",
    "cat_ftrs = ['MSZoning','Street','Alley','LandContour','LotConfig','Neighborhood','Condition1','Condition2',\\\n",
    "            'BldgType','HouseStyle','RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','Foundation',\\\n",
    "           'Heating','CentralAir','Electrical','GarageType','PavedDrive','MiscFeature','SaleType','SaleCondition']\n",
    "ordinal_ftrs = ['LotShape','Utilities','LandSlope','ExterQual','ExterCond','BsmtQual','BsmtCond','BsmtExposure',\\\n",
    "               'BsmtFinType1','BsmtFinType2','HeatingQC','KitchenQual','Functional','FireplaceQu','GarageFinish',\\\n",
    "               'GarageQual','GarageCond','PoolQC','Fence']\n",
    "ordinal_cats = [['Reg','IR1','IR2','IR3'],['AllPub','NoSewr','NoSeWa','ELO'],['Gtl','Mod','Sev'],\\\n",
    "               ['Po','Fa','TA','Gd','Ex'],['Po','Fa','TA','Gd','Ex'],['NA','Po','Fa','TA','Gd','Ex'],\\\n",
    "               ['NA','Po','Fa','TA','Gd','Ex'],['NA','No','Mn','Av','Gd'],['NA','Unf','LwQ','Rec','BLQ','ALQ','GLQ'],\\\n",
    "               ['NA','Unf','LwQ','Rec','BLQ','ALQ','GLQ'],['Po','Fa','TA','Gd','Ex'],['Po','Fa','TA','Gd','Ex'],\\\n",
    "               ['Sal','Sev','Maj2','Maj1','Mod','Min2','Min1','Typ'],['NA','Po','Fa','TA','Gd','Ex'],\\\n",
    "               ['NA','Unf','RFn','Fin'],['NA','Po','Fa','TA','Gd','Ex'],['NA','Po','Fa','TA','Gd','Ex'],\n",
    "               ['NA','Fa','TA','Gd','Ex'],['NA','MnWw','GdWo','MnPrv','GdPrv']]\n",
    "num_ftrs = ['MSSubClass','LotFrontage','LotArea','OverallQual','OverallCond','YearBuilt','YearRemodAdd',\\\n",
    "             'MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF',\\\n",
    "             'LowQualFinSF','GrLivArea','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr',\\\n",
    "             'KitchenAbvGr','TotRmsAbvGrd','Fireplaces','GarageYrBlt','GarageCars','GarageArea','WoodDeckSF',\\\n",
    "             'OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','MoSold','YrSold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# preprocess with pipeline and columntransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# one-hot encoder\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant',fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(sparse=False,handle_unknown='ignore'))])\n",
    "\n",
    "# ordinal encoder\n",
    "ordinal_transformer = Pipeline(steps=[\n",
    "    ('imputer2', SimpleImputer(strategy='constant',fill_value='NA')),\n",
    "    ('ordinal', OrdinalEncoder(categories = ordinal_cats))])\n",
    "\n",
    "# standard scaler\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# collect the transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_ftrs),\n",
    "        ('cat', categorical_transformer, cat_ftrs),\n",
    "        ('ord', ordinal_transformer, ordinal_ftrs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(876, 221)\n",
      "(292, 221)\n",
      "(292, 221)\n"
     ]
    }
   ],
   "source": [
    "# fit_transform the training set\n",
    "X_prep = preprocessor.fit_transform(X_train)\n",
    "# little hacky, but collect feature names\n",
    "feature_names = preprocessor.transformers_[0][-1] + \\\n",
    "                list(preprocessor.named_transformers_['cat'][1].get_feature_names(cat_ftrs)) + \\\n",
    "                preprocessor.transformers_[2][-1]\n",
    "\n",
    "df_train = pd.DataFrame(data=X_prep,columns=feature_names)\n",
    "print(df_train.shape)\n",
    "\n",
    "# transform the CV\n",
    "df_CV = preprocessor.transform(X_CV)\n",
    "df_CV = pd.DataFrame(data=df_CV,columns = feature_names)\n",
    "print(df_CV.shape)\n",
    "\n",
    "# transform the test\n",
    "df_test = preprocessor.transform(X_test)\n",
    "df_test = pd.DataFrame(data=df_test,columns = feature_names)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2) Continuous features: mean or median imputation\n",
    "- Imputation means you infer the missing values from the known part of the data\n",
    "- sklearn's SimpleImputer can do mean and median imputation\n",
    "- USUALLY A BAD IDEA!\n",
    "   - MCAR: mean/median of non-missing values is the same as the mean/median of the true underlying distribution, but the variances are different\n",
    "   - not MCAR: the mean/median and the variance of the completed dataset will be off\n",
    "   - supervised ML model is too confident (MCAR) or systematically off (not MCAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3) Exclude points or features with missing values\n",
    "\n",
    "- easy to do with pandas\n",
    "- it is an ACCEPTABLE approach sometimes:\n",
    "    - only small fraction of points contain missing values (maybe a few percent?)\n",
    "    - or the missing values are limited to one or a few features and a large fraction of values are missing from those features (maybe up to 90%?)\n",
    "- if the MCAR assumption is justified, dropping points will not introduce biases to your model\n",
    "- due to the smaller sample size, the confidence of your model might suffer. \n",
    "- what will you do with missing values when you deploy the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dimensions: (876, 221)\n",
      "fraction of missing values in features:\n",
      "LotFrontage    0.173516\n",
      "MasVnrArea     0.004566\n",
      "GarageYrBlt    0.050228\n",
      "dtype: float64\n",
      "fraction of points with missing values: 0.2237442922374429\n"
     ]
    }
   ],
   "source": [
    "print('data dimensions:',df_train.shape)\n",
    "perc_missing_per_ftr = df_train.isnull().sum(axis=0)/df_train.shape[0]\n",
    "print('fraction of missing values in features:')\n",
    "print(perc_missing_per_ftr[perc_missing_per_ftr > 0])\n",
    "frac_missing = sum(df_train.isnull().sum(axis=1)!=0)/df_train.shape[0]\n",
    "print('fraction of points with missing values:',frac_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(876, 221)\n",
      "(680, 221)\n",
      "(876, 218)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "# by default, rows/points are dropped\n",
    "df_r = df_train.dropna()\n",
    "print(df_r.shape)\n",
    "# drop features with missing values\n",
    "df_c = df_train.dropna(axis=1)\n",
    "print(df_c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4) Multivariate Imputation\n",
    "\n",
    "- **Does it make sense to impute the values?**\n",
    "    - GarageYearBuilt should not be imputed because a missing value indicates no garage on the property\n",
    "- models each feature with missing values as a function of other features, and uses that estimate for imputation\n",
    "   - at each step, a feature is designated as target variable and the other feature columns are treated as feature matrix X\n",
    "   - a regressor is trained on (X, y) for known y\n",
    "   - then, the regressor is used to predict the missing values of y\n",
    "- in the ML pipeline:\n",
    "   - create n imputed datasets\n",
    "   - run all of them through the ML pipeline\n",
    "   - generate n test scores\n",
    "   - the uncertainty in the test scores is due to the uncertainty in imputation\n",
    "- works on MCAR and MAR, fails on MNAR\n",
    "- paper [here](https://www.jstatsoft.org/article/view/v045i03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### sklearn's IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LotFrontage  MasVnrArea  GarageYrBlt\n",
      "0     0.424926   -0.573303     0.979398\n",
      "1          NaN    0.492835     1.018748\n",
      "2          NaN   -0.573303     0.192399\n",
      "3    -0.049970    0.810076    -0.476551\n",
      "4    -1.474659   -0.022031     0.979398\n",
      "   LotFrontage  MasVnrArea  GarageYrBlt\n",
      "0     0.424926   -0.573303     0.979398\n",
      "1    -1.289018    0.492835     1.018748\n",
      "2     0.014788   -0.573303     0.192399\n",
      "3    -0.049970    0.810076    -0.476551\n",
      "4    -1.474659   -0.022031     0.979398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/azsom/opt/anaconda3/envs/data1030/lib/python3.9/site-packages/sklearn/impute/_iterative.py:685: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\"[IterativeImputer] Early stopping criterion not\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "print(df_train[['LotFrontage','MasVnrArea','GarageYrBlt']].head())\n",
    "\n",
    "imputer = IterativeImputer(estimator = RandomForestRegressor(n_estimators=10), random_state=1000)\n",
    "X_impute = imputer.fit_transform(df_train)\n",
    "df_train_imp = pd.DataFrame(data=X_impute, columns = df_train.columns)\n",
    "\n",
    "print(df_train_imp[['LotFrontage','MasVnrArea','GarageYrBlt']].head())\n",
    "\n",
    "df_CV_imp = pd.DataFrame(data=imputer.transform(df_CV), columns = df_train.columns)\n",
    "df_test_imp = pd.DataFrame(data=imputer.transform(df_test), columns = df_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='LIGHTGRAY'>Missing data, part 2</font>\n",
    "\n",
    "<font color='LIGHTGRAY'>By the end of this lecture, you will be able to</font>\n",
    "- <font color='LIGHTGRAY'>review simple approaches for handling missing values</font>\n",
    "- **Apply XGBoost to a dataset with missing values**\n",
    "- <font color='LIGHTGRAY'>Apply the reduced-features model (also called the pattern submodel approach)</font>\n",
    "- <font color='LIGHTGRAY'>Decide which approach is best for your dataset</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## XGBoost\n",
    "- eXtreme Gradient Boosting - a popular tree-based method\n",
    "- [blog post](https://xgboost.readthedocs.io/en/latest/tutorials/model.html) and [paper](http://delivery.acm.org/10.1145/2940000/2939785/p785-chen.pdf)\n",
    "- more advanced than random forest\n",
    "   - it has l1 and l2 regularization while random forest does not\n",
    "   - trees are not independent\n",
    "      - the next tree is built to improve the previous tree\n",
    "      - less trees are necessary to achieve same accuracy\n",
    "      - but XGBoost trees can overfit - more on this in the problem set\n",
    "   - handles missing values well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## XGBoost and missing values\n",
    "- sklearn raises an error if the feature matrix (X) contains nans. \n",
    "- XGBoost doesn't! \n",
    "- If a feature with missing values is split:\n",
    "    - XGBoost tries to put the points with missing values to the left and right\n",
    "    - calculates the impurity measure for both options\n",
    "    - puts the points with missing values to the side with the lower impurity\n",
    "- if missingness correlates with the target variable, XGBoost extracts this info!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the CV RMSE: 23470.132687324658\n",
      "the test RMSE: 31748.96283078089\n",
      "the test R2: 0.8540372805542484\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "param_grid = {\"learning_rate\": [0.03],\n",
    "              \"n_estimators\": [10000],\n",
    "              \"seed\": [0],\n",
    "              #\"reg_alpha\": [0e0, 1e-2, 1e-1, 1e0, 1e1, 1e2],\n",
    "              #\"reg_lambda\": [0e0, 1e-2, 1e-1, 1e0, 1e1, 1e2],\n",
    "              \"missing\": [np.nan], \n",
    "              #\"max_depth\": [1,3,10,30,100],\n",
    "              \"colsample_bytree\": [0.9],              \n",
    "              \"subsample\": [0.66]}\n",
    "\n",
    "XGB = xgboost.XGBRegressor()\n",
    "XGB.set_params(**ParameterGrid(param_grid)[0])\n",
    "XGB.fit(df_train,y_train,early_stopping_rounds=50,eval_set=[(df_CV, y_CV)], verbose=False)\n",
    "y_CV_pred = XGB.predict(df_CV)\n",
    "print('the CV RMSE:',np.sqrt(mean_squared_error(y_CV,y_CV_pred)))\n",
    "y_test_pred = XGB.predict(df_test)\n",
    "print('the test RMSE:',np.sqrt(mean_squared_error(y_test,y_test_pred)))\n",
    "print('the test R2:',r2_score(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### XGBoost with the imputed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the CV RMSE: 23750.609305517548\n",
      "the test RMSE: 33261.94619418601\n",
      "the test R2: 0.8397942222330688\n"
     ]
    }
   ],
   "source": [
    "XGB.fit(df_train_imp,y_train,early_stopping_rounds=50,eval_set=[(df_CV_imp, y_CV)], verbose=False)\n",
    "y_CV_pred = XGB.predict(df_CV_imp)\n",
    "print('the CV RMSE:',np.sqrt(mean_squared_error(y_CV,y_CV_pred)))\n",
    "y_test_pred = XGB.predict(df_test_imp)\n",
    "print('the test RMSE:',np.sqrt(mean_squared_error(y_test,y_test_pred)))\n",
    "print('the test R2:',r2_score(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mudcard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
